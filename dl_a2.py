# -*- coding: utf-8 -*-
"""DL_A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ernestsoo/twitter-stance-classification/blob/Raj/DL_A2.ipynb
"""

import tensorflow as tf
AUTOTUNE = tf.data.experimental.AUTOTUNE
import numpy as np
import pandas as pd

import tensorflow_datasets as tfds
import pathlib
import shutil
import tempfile

from  IPython import display
from matplotlib import pyplot as plt

"""#Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
logdir = pathlib.Path(tempfile.mkdtemp())/"tensorboard_logs"
shutil.rmtree(logdir, ignore_errors=True)

# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Open an embedded TensorBoard viewer
# %tensorboard --logdir {logdir}/models

"""#Plotting function"""

from itertools import cycle
def plotter(history_hold, metric = 'binary_crossentropy', ylim=[0.0, 1.0]):
  cycol = cycle('bgrcmk')
  for name, item in history_hold.items():
    y_train = item.history[metric]
    y_val = item.history['val_' + metric]
    x_train = np.arange(0,len(y_val))

    c=next(cycol)

    plt.plot(x_train, y_train, c+'-', label=name+'_train')
    plt.plot(x_train, y_val, c+'--', label=name+'_val')

  plt.legend()
  plt.xlim([1, max(plt.xlim())])
  plt.ylim(ylim)
  plt.xlabel('Epoch')
  plt.ylabel(metric)
  plt.grid(True)

"""#Importing the dataset"""

from google.colab import drive
drive.mount('/content/drive')

!cp /content/drive/'My Drive'/StanceDataset.zip .
!unzip -q -o StanceDataset.zip
!rm StanceDataset.zip

!cp /content/drive/'My Drive'/glove.twitter.27B.200d.txt .

# Read train and test dataset.
with open('StanceDataset/train.csv','r',encoding='ISO-8859-1') as f:
    train = pd.read_csv(f)

with open('StanceDataset/test.csv','r',encoding='ISO-8859-1') as y:
    test = pd.read_csv(y)
    
    
f.close()
y.close()

train.head()

test.head()

train["Stance"].value_counts()

train["Target"].value_counts()

"""Encoding the target varibale"""

encode = {
    "AGAINST" : 0,
    "NONE"  : 1,       
    "FAVOR" : 2
}

train["Stance"] = train["Stance"].apply(lambda x: encode[x])
train.head()

train["Target"].value_counts()

# We use this to split hashtags
pip install wordsegment

# To correct the spellings
pip install autocorrect

"""#Tweet Preprocessing"""

import string
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re
from wordsegment import load, segment
load()
from autocorrect import Speller

spell = Speller(lang='en')
tweets = list()
target = list()
hashtag = list()

#extracting hashtag keywords from tweet
train['hashtag'] = train['Tweet'].apply(lambda x: re.findall(r'\B#\w*[a-zA-Z]+\w*', x))

#extracting tagged users from tweet
train['users'] = train['Tweet'].apply(lambda x: re.findall(r'\B@\w*[a-zA-Z]+\w*', x))


lines_tweet = train["Tweet"].values.tolist()
lines_target = train["Target"].values.tolist()

for line in lines_tweet:


     
    #remove puntuations
    
    line = ' '.join(re.sub('([^0-9A-Za-z \t])|(\w+:\/\/\S+)', ' ', line).split())

    # tokenize the tweet
    tokens = word_tokenize(line)

    # convert to lower case
    tokens = [w.lower() for w in tokens]


    tweets.append(tokens)

#target processing
for line in lines_target:
    
    tokens = word_tokenize(line)

    # convert to lower case
    tokens = [w.lower() for w in tokens]
    
    target.append(tokens)

#Processing hashtags
lines_hashtag = train["hashtag"].values.tolist()


for line in lines_hashtag:
  line = ' '.join(line)
  line = ' '.join(re.sub('([^0-9A-Za-z \t])|(\w+:\/\/\S+)', ' ', line).split())
  line = ' '.join(segment(line))
  tokens = word_tokenize(line)

    # convert to lower case
  tokens = [w.lower() for w in tokens]
    
  hashtag.append(tokens)

hashtag[:5]

train.head()

len(max(tweets,key = len))

len(max(target,key = len))

from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

validation_split = 0.20
max_length_tweet = len(max(tweets,key = len))
max_length_target = len(max(target,key = len))
max_length_hashtag = len(max(hashtag,key = len))

tokenizer_obj_tweet = Tokenizer()
tokenizer_obj_tweet.fit_on_texts(tweets)
sequences_tweets = tokenizer_obj_tweet.texts_to_sequences(tweets)

tokenizer_obj_target = Tokenizer()
tokenizer_obj_target.fit_on_texts(target)
sequences_target = tokenizer_obj_target.texts_to_sequences(target)

tokenizer_obj_hashtag = Tokenizer()
tokenizer_obj_hashtag.fit_on_texts(hashtag)
sequences_hashtag = tokenizer_obj_hashtag.texts_to_sequences(hashtag)

word_index_tweet = tokenizer_obj_tweet.word_index
print("Tweet unique tokens - "+str(len(word_index_tweet)))
vocab_size_tweet = len(tokenizer_obj_tweet.word_index) + 1
print('Tweet vocab_size - '+str(vocab_size_tweet))


word_index_target = tokenizer_obj_target.word_index
print("Target unique tokens - "+str(len(word_index_target)))
vocab_size_target = len(tokenizer_obj_target.word_index) + 1
print('Target vocab_size - '+str(vocab_size_target))


word_index_hashtag = tokenizer_obj_hashtag.word_index
print("Hashtag unique tokens - "+str(len(word_index_hashtag)))
vocab_size_hashtag = len(tokenizer_obj_hashtag.word_index) + 1
print('Hashtag vocab_size - '+str(vocab_size_hashtag))

lines_pad_tweets = pad_sequences(sequences_tweets, maxlen=max_length_tweet, padding='post')
lines_pad_target = pad_sequences(sequences_target, maxlen=max_length_target, padding='post')
lines_pad_hashtag = pad_sequences(sequences_hashtag, maxlen=max_length_hashtag, padding='post')

category = train['Stance'].values

indices = np.arange(lines_pad_tweets.shape[0])
np.random.shuffle(indices)
lines_pad_tweets = lines_pad_tweets[indices]
lines_pad_target = lines_pad_target[indices]
lines_pad_hashtag = lines_pad_hashtag[indices]
category = category[indices]

n_values = np.max(category) + 1
Y = np.eye(n_values)[category]

num_validation_samples = int(validation_split * lines_pad_tweets.shape[0])

X_train_tweets_pad = lines_pad_tweets[:-num_validation_samples]
X_train_target_pad = lines_pad_target[:-num_validation_samples]
X_train_hashtag_pad = lines_pad_hashtag[:-num_validation_samples]
y_train = Y[:-num_validation_samples]
X_val_tweets_pad = lines_pad_tweets[-num_validation_samples:]
X_val_target_pad = lines_pad_target[-num_validation_samples:]
X_val_hashtag_pad = lines_pad_hashtag[-num_validation_samples:]
y_val = Y[-num_validation_samples:]

# Randomly sample some train data
train_len = X_train_tweets_pad.shape[0]

idx = np.random.randint(train_len, size=train_len//25)

X_train_tweets_pad_sampled = X_train_tweets_pad[idx, :]
X_train_target_pad_sampled = X_train_tweets_pad[idx, :]
y_train_sampled = y_train[idx]

print('Shape of X_train_tweet_pad:', X_train_tweets_pad.shape)
print('Shape of X_train_target_pad:', X_train_target_pad.shape)
print('Shape of X_train_hashtag_pad:', X_train_hashtag_pad.shape)
print('Shape of y_train:', y_train.shape)

print('Shape of X_train_tweet_pad_sampled:', X_train_tweets_pad_sampled.shape)
print('Shape of X_train_target_pad_sampled:', X_train_target_pad_sampled.shape)
print('Shape of y_train_sampled:', y_train_sampled.shape)

print('Shape of X_test_tweet_pad:', X_val_tweets_pad.shape)
print('Shape of X_test_target_pad:', X_val_target_pad.shape)
print('Shape of X_test_hashtag_pad:', X_val_hashtag_pad.shape)
print('Shape of y_test:', y_val.shape)

file = open('glove.twitter.27B.200d.txt', encoding='utf-8')

glove_vectors = dict()
for line in file:
  values = line.split()
  word = values[0]
  features = np.asarray(values[1:])
  glove_vectors[word] = features

file.close()

embedding_dim = 200

def get_callbacks(name):
  return [
    tf.keras.callbacks.TensorBoard(logdir/name, histogram_freq=1),
  ]

E_T_tweet = np.zeros((len(word_index_tweet) + 1, embedding_dim))
for word, i in word_index_tweet.items():
    embedding_vector = glove_vectors.get(word)
    if embedding_vector is not None:
        E_T_tweet[i] = embedding_vector

E_T_target = np.zeros((len(word_index_target) + 1, embedding_dim))
for word, i in word_index_target.items():
    embedding_vector = glove_vectors.get(word)
    if embedding_vector is not None:
        E_T_target[i] = embedding_vector

E_T_hashtag = np.zeros((len(word_index_hashtag) + 1, embedding_dim))
for word, i in word_index_hashtag.items():
    embedding_vector = glove_vectors.get(word)
    if embedding_vector is not None:
        E_T_hashtag[i] = embedding_vector

del glove_vectors

m_histories = {}
def get_callbacks(name, early_stop=True):
  if early_stop:
    return [
            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25),
            tf.keras.callbacks.TensorBoard(logdir/name, histogram_freq=60, embeddings_freq=60),
            

            ]
  else:
    return [tf.keras.callbacks.TensorBoard(logdir/name)]

"""Creating an optimizer function."""

from tensorflow.keras.optimizers import Adam, SGD

epochs =200
BATCH_SIZE = 64
def optimizer_adam(dec_rate = 30,lr = 0.001):
  STEPS_PER_EPOCH = X_train_tweets_pad.shape[0]//BATCH_SIZE
  lr_adam = lr

  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  lr_adam,
  decay_steps=STEPS_PER_EPOCH*1000,
  decay_rate=dec_rate,
  staircase=False)
  opt_adam = Adam(learning_rate=lr_schedule)
  return opt_adam

import tensorflow.keras as keras
from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional, Attention
from tensorflow.keras.models import Sequential

"""#Model 1 Keras Attention layer
In this we use a predefined keras Attention layer to capture attention two independent bidirectional lstm of tweet and target.
"""

def get_model(embedding_dim = 100, dropout_rate = 0.1,rec_dropout = 0.1, reg_lambda= 0.001):


  input_tweets = keras.Input(shape=(X_train_tweets_pad.shape[1],))
  input_target = keras.Input(shape=(X_train_target_pad.shape[1],))


  input_tweet_length = X_train_tweets_pad.shape[1]
  input_target_length = X_train_target_pad.shape[1]

#################
#Encoding Tweets
  x = tf.keras.layers.Embedding(len(word_index_tweet) + 1,
                      embedding_dim,
                      weights=[E_T_tweet],
                      input_length=max_length_tweet,
                      trainable=False)(input_tweets)


  x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=False, 
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))(x)
  x = keras.Model(inputs = input_tweets,outputs = x)

##################### 
#Encoding Targets
  y = tf.keras.layers.Embedding(len(word_index_target) + 1,
                      embedding_dim,
                      weights=[E_T_target],
                      input_length=max_length_target,
                      trainable=False)(input_target)


  y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=False, 
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))(y)

  y = keras.Model(inputs = input_target,outputs = y)

######################
#Keras Attention layer
  query_value_attention_seq = tf.keras.layers.Attention()([x.output, y.output])

 

 
  m = tf.keras.layers.Dense(64, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(query_value_attention_seq)

  m = tf.keras.layers.Dense(3, activation="softmax")(m)



  model = keras.Model(inputs=[x.input, y.input],outputs = m)

  
  
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer_adam(),
                metrics=['accuracy'])
  
  return model

model_glove100 = get_model()



m_histories['glove100'] = model_glove100.fit(
                  x=[X_train_tweets_pad,X_train_target_pad],
                  y=y_train,
                  validation_data=([X_val_tweets_pad,X_val_target_pad],y_val),
                  epochs=100,
                  batch_size=64,
                  verbose=1,
                  shuffle=True,
                  callbacks=get_callbacks('models/model_glove100'))

plotter(m_histories, ylim=[0.0, 2], metric = 'loss')

"""#Model 2: Bidirectional LSTM of target conditioned on tweet with Custom Attention layer"""

def get_model2(embedding_dim = 100, dropout_rate = 0.1,rec_dropout = 0.1, reg_lambda= 0.001):


  input_tweets = keras.Input(shape=(X_train_tweets_pad.shape[1],))
  input_target = keras.Input(shape=(X_train_target_pad.shape[1],))


  input_tweet_length = X_train_tweets_pad.shape[1]
  input_target_length = X_train_target_pad.shape[1]

 #Tweet embedding layer
  x = tf.keras.layers.Embedding(len(word_index_tweet) + 1,
                      embedding_dim,
                      weights=[E_T_tweet],
                      input_length=max_length_tweet,
                      trainable=False)(input_tweets)

  #Bidirectional LSTM encoding of tweet
  x_encode = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=False, 
                                    return_state = True, 
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))
  
 

  (x_encode, target_fw_state_h, target_fw_state_s,target_bw_state_h, target_bw_state_s) = x_encode(x)

  y = tf.keras.layers.Embedding(len(word_index_target) + 1,
                      embedding_dim,
                      weights=[E_T_target],
                      input_length=max_length_target,
                      trainable=False)(input_target)

  #Conditional encoding of target on tweet
  y_encode = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=True, 
                                    return_state = False, 
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))

  y_encoder_out = y_encode(y, initial_state=[target_fw_state_h, target_fw_state_s,target_bw_state_h, target_bw_state_s])
  

  attention = tf.keras.layers.Dense(1, activation='tanh')(y_encoder_out)
  attention = tf.keras.layers.Flatten()(attention)
  attention = tf.keras.layers.Activation('softmax')(attention)
  attention = tf.keras.layers.RepeatVector(64)(attention)
  attention = tf.keras.layers.Permute([2, 1])(attention)

  sent_representation = tf.keras.layers.Concatenate()([y_encoder_out, attention])
  sent_representation = tf.keras.layers.Lambda(lambda xin: tf.keras.backend.sum(xin, axis=-2), output_shape=(64,))(sent_representation)
 
  m = tf.keras.layers.Dense(64, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(sent_representation)

  m = tf.keras.layers.Dense(3, activation="softmax")(m)



  model = keras.Model(inputs=[input_tweets, input_target],outputs = m)

  
  
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer_adam(),
                metrics=['accuracy'])
  
  return model

model_conditional = get_model2()



m_histories['conditional'] = model_conditional.fit(
                  x=[X_train_tweets_pad,X_train_target_pad],
                  y=y_train,
                  validation_data=([X_val_tweets_pad,X_val_target_pad],y_val),
                  epochs=100,
                  batch_size=64,
                  verbose=1,
                  shuffle=True,
                  callbacks=get_callbacks('models/conditional'))

plotter(m_histories, ylim=[0.0, 2], metric = 'loss')

"""#Model 3: Target augmented embedding with Attention layer + Tweet LSTM Encoding
As implemented in **Stance Detection with Bidirectional Conditional Encoding Augenstein et al., 2016**
"""

class Linear(keras.layers.Layer):
    def __init__(self, units=34, input_dim=64):
        super(Linear, self).__init__()
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value=w_init(shape=(input_dim, units), dtype="float32"),
            trainable=True,
        )
        b_init = tf.zeros_initializer()
        self.b = tf.Variable(
            initial_value=b_init(shape=(units,), dtype="float32"), trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

def get_model3(embedding_dim = 100, dropout_rate = 0.1,rec_dropout = 0.1, reg_lambda= 0.0001):


  input_tweets = keras.Input(shape=(X_train_tweets_pad.shape[1],))
  input_target = keras.Input(shape=(X_train_target_pad.shape[1],))


  input_tweet_length = X_train_tweets_pad.shape[1]
  input_target_length = X_train_target_pad.shape[1]

  #Target embedding layer
  x = tf.keras.layers.Embedding(len(word_index_target) + 1,
                      embedding_dim,
                      weights=[E_T_target],
                      input_length=max_length_target,
                      trainable=False)(input_target)

  #Mean of target embedding layer
  x = keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1))(x)
 
  print("shape after mean layer:",x.shape)

  x = tf.keras.layers.Reshape((1,100))(x)

  #Tweet embedding layer
  y = tf.keras.layers.Embedding(len(word_index_tweet) + 1,
                      embedding_dim,
                      weights=[E_T_tweet],
                      input_length=max_length_tweet,
                      trainable=False)(input_tweets)

  print("shape of tweet emb layer:",y.shape)
  
  #Concatenating tweet and target mean
  y_x = tf.keras.layers.concatenate([y,x],axis=1)
  print("shape after concat layer:",y_x.shape)

  #Linear transform to add attention weights.
  y_x_linear = Linear(1,100)(y_x)
  print("shape after linear layer:",y_x_linear.shape)


  #Flattening the matrix
  y_x_linear = tf.keras.layers.Flatten()(y_x_linear)
  print("shape after Flatten layer:",y_x_linear.shape)

  #Softmax transformation
  y_x_linear_softmax = tf.keras.layers.Softmax()(y_x_linear)
  print("shape after softmax transformation layer:",y_x_linear_softmax.shape)

  y_x_linear_softmax = tf.keras.layers.Reshape((34,1))(y_x_linear_softmax) 
  print("shape after softmax reshape layer:",y_x_linear_softmax.shape)


  #Bidirectional LSTM encoding of tweet
  y_encode = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=False,  
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))(y)
  print("shape after lstm tweet encode layer:",y_encode.shape)


  y_encode = tf.keras.layers.Reshape((1,128))(y_encode) 
  print("shape after lstm tweet reshape layer:",y_encode.shape)

  #Inner product layer
  dot = tf.keras.layers.Dot(axes=(1,2))([y_encode,y_x_linear_softmax])

  print("shape after product:",dot.shape)
  
  dot_flatten = tf.keras.layers.Flatten()(dot)
  print("shape after dot Flatten layer:",dot_flatten.shape)
  
  m = tf.keras.layers.BatchNormalization(momentum=.4)(dot_flatten)
  m = tf.keras.layers.Dropout(dropout_rate)(m)

  m = tf.keras.layers.Dense(128, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),kernel_initializer="he_normal")(m)
  #m = tf.keras.layers.BatchNormalization(momentum=.4)(m)
  

  #m = tf.keras.layers.Dense(64, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),kernel_initializer="he_normal")(m)
 

  m = tf.keras.layers.Dense(3, activation="softmax")(m)



  model = keras.Model(inputs=[input_tweets, input_target],outputs = m)

  
  
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer_adam(),
                metrics=['accuracy'])
  
  return model

model_aug = get_model3()



m_histories['Augmented'] = model_aug.fit(
                  x=[X_train_tweets_pad,X_train_target_pad],
                  y=y_train,
                  validation_data=([X_val_tweets_pad,X_val_target_pad],y_val),
                  epochs=100,
                  batch_size=64,
                  verbose=1,
                  shuffle=True,
                  callbacks=get_callbacks('models/aug'))

plotter(m_histories, ylim=[0.0, 2], metric = 'loss')

"""#Model 4: Bidirectional LSTM encoding on Target Augmented embedding.
We take the mean of target vector and concatenaes it with tweet and then applies conditonal encoding and then attention layer.
"""

def get_model4(embedding_dim = 100, dropout_rate = 0.1,rec_dropout = 0.1, reg_lambda= 0.0001):


  input_tweets = keras.Input(shape=(X_train_tweets_pad.shape[1],))
  input_target = keras.Input(shape=(X_train_target_pad.shape[1],))


  input_tweet_length = X_train_tweets_pad.shape[1]
  input_target_length = X_train_target_pad.shape[1]


 ##########################
 #Target Embedding layer
  x = tf.keras.layers.Embedding(len(word_index_target) + 1,
                      embedding_dim,
                      weights=[E_T_target],
                      input_length=max_length_target,
                      trainable=False)(input_target)

  #Taking mean of the target embedding vector.
  x = keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1))(x)
 
  print("shape after mean layer:",x.shape)

  x = tf.keras.layers.Reshape((1,100))(x)

  #Tweet Embedding layer
  y = tf.keras.layers.Embedding(len(word_index_tweet) + 1,
                      embedding_dim,
                      weights=[E_T_tweet],
                      input_length=max_length_tweet,
                      trainable=False)(input_tweets)

  print("shape of tweet emb layer:",y.shape)
  
  #Concatenating target mean and tweet
  y_x = tf.keras.layers.concatenate([y,x],axis=1)
  print("shape after concat layer:",y_x.shape)


  #Bidirectional LSTM encoder on concatenated layer.
  y_encode = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=True,  
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))(y_x)
  print("shape after lstm tweet encode layer:",y_encode.shape)

  #Attention Mechanism
  attention = tf.keras.layers.Dense(1, activation='tanh')(y_encode)
  attention = tf.keras.layers.Flatten()(attention)
  attention = tf.keras.layers.Activation('softmax')(attention)
  attention = tf.keras.layers.RepeatVector(64)(attention)
  attention = tf.keras.layers.Permute([2, 1])(attention)

  sent_representation = tf.keras.layers.Concatenate()([y_encode, attention])
  sent_representation = tf.keras.layers.Lambda(lambda xin: tf.keras.backend.sum(xin, axis=-2), output_shape=(64,))(sent_representation)
  

  #Fully connected layers
  m = tf.keras.layers.Dense(128, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(sent_representation)
  m = tf.keras.layers.BatchNormalization(momentum=.4)(m)
  m = tf.keras.layers.Dropout(dropout_rate)(m)
  m = tf.keras.layers.Dense(64, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(m)
  m = tf.keras.layers.Dense(3, activation="softmax")(m)



  model = keras.Model(inputs=[input_tweets, input_target],outputs = m)

  
  
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer_adam(),
                metrics=['accuracy'])
  
  return model

model_aug_4 = get_model4()



m_histories['Augmented4'] = model_aug_4.fit(
                  x=[X_train_tweets_pad,X_train_target_pad],
                  y=y_train,
                  validation_data=([X_val_tweets_pad,X_val_target_pad],y_val),
                  epochs=100,
                  batch_size=64,
                  verbose=1,
                  shuffle=True,
                  callbacks=get_callbacks('models/augmentation4'))

plotter(m_histories, ylim=[0.0, 2], metric = 'loss')

"""#Model 5: Tweet conditionally encoded on (Target and Hashtags)"""

def get_model5(embedding_dim = 200, dropout_rate = 0.45,rec_dropout = 0.45, reg_lambda= 0.0025):


  input_tweets = keras.Input(shape=(X_train_tweets_pad.shape[1],))
  input_target = keras.Input(shape=(X_train_target_pad.shape[1],))
  input_hashtag = keras.Input(shape=(X_train_hashtag_pad.shape[1],))

  input_tweet_length = X_train_tweets_pad.shape[1]
  input_target_length = X_train_target_pad.shape[1]
  input_hashtag_length = X_train_hashtag_pad.shape[1]

 
  #Target Embedding layer
  x = tf.keras.layers.Embedding(len(word_index_target) + 1,
                      embedding_dim,
                      weights=[E_T_target],
                      input_length=max_length_target,
                      trainable=False)(input_target)

  #Hashtag Embedding layer
  z = tf.keras.layers.Embedding(len(word_index_hashtag) + 1,
                      embedding_dim,
                      weights=[E_T_hashtag],
                      input_length=max_length_hashtag,
                      trainable=False)(input_hashtag)

  #Mean weight of Hashtag
  z = keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1))(z)
  print("shape after mean layer:",z.shape)

  z = tf.keras.layers.Reshape((1,200))(z)

  #Concatenating hashtag mean weight and target
  x_z = tf.keras.layers.concatenate([x,z],axis=1)
  print("shape after concat layer:",x_z.shape)


  #encoding the concatenated layer using bidirectional lstm
  x_z_encode = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=False, 
                                    return_state = True, 
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))
  
 

  (x_z_encode, target_fw_state_h, target_fw_state_s,target_bw_state_h, target_bw_state_s) = x_z_encode(x_z)

  
  
  
  #Tweet embedding layer
  y = tf.keras.layers.Embedding(len(word_index_tweet) + 1,
                      embedding_dim,
                      weights=[E_T_tweet],
                      input_length=max_length_tweet,
                      trainable=False)(input_tweets)

  #Tweet encoding layer
  y_encode = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=True, 
                                    return_state = False, 
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))

  y_encoder_out = y_encode(y, initial_state=[target_fw_state_h, target_fw_state_s,target_bw_state_h, target_bw_state_s])


  #Attention mechanism
  attention = tf.keras.layers.Dense(1, activation='tanh')(y_encoder_out)
  attention = tf.keras.layers.Flatten()(attention)
  attention = tf.keras.layers.Activation('softmax')(attention)
  attention = tf.keras.layers.RepeatVector(64)(attention)
  attention = tf.keras.layers.Permute([2, 1])(attention)

  sent_representation = tf.keras.layers.Concatenate()([y_encoder_out, attention])
  sent_representation = tf.keras.layers.Lambda(lambda xin: tf.keras.backend.sum(xin, axis=-2), output_shape=(64,))(sent_representation)
  

  #Fully connected layers
  m = tf.keras.layers.Dense(128, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(sent_representation)
  m = tf.keras.layers.BatchNormalization(momentum=.4)(m)
  m = tf.keras.layers.Dropout(dropout_rate)(m)
  m = tf.keras.layers.Dense(64, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(m)
  m = tf.keras.layers.Dense(3, activation="softmax")(m)



  model = keras.Model(inputs=[input_tweets, input_target,input_hashtag],outputs = m)

  
  
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer_adam(lr = 0.0004),
                metrics=['accuracy'])
  
  return model

model_hashtag_5 = get_model5()



m_histories['hashtag5'] = model_hashtag_5.fit(
                  x=[X_train_tweets_pad,X_train_target_pad,X_train_hashtag_pad],
                  y=y_train,
                  validation_data=([X_val_tweets_pad,X_val_target_pad,X_val_hashtag_pad],y_val),
                  epochs=200,
                  batch_size=64,
                  verbose=1,
                  shuffle=True,
                  callbacks=get_callbacks('models/hashtag5'))

plotter(m_histories, ylim=[0.0, 2], metric = 'loss')

def get_model6(embedding_dim = 200, dropout_rate = 0.45,rec_dropout = 0.45, reg_lambda= 0.0025):


  input_tweets = keras.Input(shape=(X_train_tweets_pad.shape[1],))
  input_target = keras.Input(shape=(X_train_target_pad.shape[1],))
  input_hashtag = keras.Input(shape=(X_train_hashtag_pad.shape[1],))

  input_tweet_length = X_train_tweets_pad.shape[1]
  input_target_length = X_train_target_pad.shape[1]
  input_hashtag_length = X_train_hashtag_pad.shape[1]

 
  x = tf.keras.layers.Embedding(len(word_index_target) + 1,
                      embedding_dim,
                      weights=[E_T_target],
                      input_length=max_length_target,
                      trainable=False)(input_target)

  z = tf.keras.layers.Embedding(len(word_index_hashtag) + 1,
                      embedding_dim,
                      weights=[E_T_hashtag],
                      input_length=max_length_hashtag,
                      trainable=False)(input_hashtag)

  z = keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1))(z)
  print("shape after mean layer:",z.shape)

  z = tf.keras.layers.Reshape((1,200))(z)

  x_z = tf.keras.layers.concatenate([x,z],axis=1)
  print("shape after concat layer:",x_z.shape)



  x_z_encode = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=False, 
                                    return_state = True, 
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))
  
 

  (x_z_encode, target_fw_state_h, target_fw_state_s,target_bw_state_h, target_bw_state_s) = x_z_encode(x_z)

  
  
  

  y = tf.keras.layers.Embedding(len(word_index_tweet) + 1,
                      embedding_dim,
                      weights=[E_T_tweet],
                      input_length=max_length_tweet,
                      trainable=False)(input_tweets)

  y_encode = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=True, 
                                    return_state = False, 
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))

  y_encoder_out = y_encode(y, initial_state=[target_fw_state_h, target_fw_state_s,target_bw_state_h, target_bw_state_s])


  attention = tf.keras.layers.Dense(1, activation='tanh')(y_encoder_out)
  attention = tf.keras.layers.Flatten()(attention)
  attention = tf.keras.layers.Activation('softmax')(attention)
  attention = tf.keras.layers.RepeatVector(64)(attention)
  attention = tf.keras.layers.Permute([2, 1])(attention)

  sent_representation = tf.keras.layers.Concatenate()([y_encoder_out, attention])
  m = tf.keras.layers.Lambda(lambda xin: tf.keras.backend.sum(xin, axis=-2), output_shape=(64,))(sent_representation)
  
  m = tf.keras.layers.BatchNormalization(momentum=.4)(m)
  m = tf.keras.layers.Dropout(dropout_rate)(m)

  m = tf.keras.layers.Dense(256, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(m)
  m = tf.keras.layers.BatchNormalization(momentum=.4)(m)
  m = tf.keras.layers.Dropout(dropout_rate)(m)
  m = tf.keras.layers.Dense(128, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(m)
  m = tf.keras.layers.Dense(3, activation="softmax")(m)



  model = keras.Model(inputs=[input_tweets, input_target,input_hashtag],outputs = m)

  
  
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer_adam(lr = 0.0004),
                metrics=['accuracy'])
  
  return model

model_hashtag_6 = get_model6()



m_histories['hashtag6'] = model_hashtag_6.fit(
                  x=[X_train_tweets_pad,X_train_target_pad,X_train_hashtag_pad],
                  y=y_train,
                  validation_data=([X_val_tweets_pad,X_val_target_pad,X_val_hashtag_pad],y_val),
                  epochs=200,
                  batch_size=64,
                  verbose=1,
                  shuffle=True,
                  callbacks=get_callbacks('models/hashtag6'))

plotter(m_histories, ylim=[0.0, 4], metric = 'loss')

"""#Model 7: Bidirectional LSTM of tweet conditioned on target with Attention layer"""

def get_model7(embedding_dim = 200, dropout_rate = 0.1,rec_dropout = 0.1, reg_lambda= 0.0001):


  input_tweets = keras.Input(shape=(X_train_tweets_pad.shape[1],))
  input_target = keras.Input(shape=(X_train_target_pad.shape[1],))


  input_tweet_length = X_train_tweets_pad.shape[1]
  input_target_length = X_train_target_pad.shape[1]

 
  x = tf.keras.layers.Embedding(len(word_index_target) + 1,
                      embedding_dim,
                      weights=[E_T_target],
                      input_length=max_length_target,
                      trainable=False)(input_target)


  x = keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1))(x)
 
  print("shape after mean layer:",x.shape)

  x = tf.keras.layers.Reshape((1,100))(x)


  y = tf.keras.layers.Embedding(len(word_index_tweet) + 1,
                      embedding_dim,
                      weights=[E_T_tweet],
                      input_length=max_length_tweet,
                      trainable=False)(input_tweets)

  print("shape of tweet emb layer:",y.shape)
  
  y_x = tf.keras.layers.concatenate([y,x],axis=1)
  print("shape after concat layer:",y_x.shape)


  y_encode = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  
                                    return_sequences=True,  
                                    kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                    dropout=dropout_rate,
                                    recurrent_dropout = rec_dropout))(y_x)
  print("shape after lstm tweet encode layer:",y_encode.shape)


  attention = tf.keras.layers.Dense(1, activation='tanh')(y_encode)
  attention = tf.keras.layers.Flatten()(attention)
  attention = tf.keras.layers.Activation('softmax')(attention)
  attention = tf.keras.layers.RepeatVector(64)(attention)
  attention = tf.keras.layers.Permute([2, 1])(attention)

  sent_representation = tf.keras.layers.Concatenate()([y_encode, attention])
  sent_representation = tf.keras.layers.Lambda(lambda xin: tf.keras.backend.sum(xin, axis=-2), output_shape=(64,))(sent_representation)
  


  m = tf.keras.layers.Dense(128, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(sent_representation)
  m = tf.keras.layers.BatchNormalization(momentum=.4)(m)
  m = tf.keras.layers.Dropout(dropout_rate)(m)
  m = tf.keras.layers.Dense(64, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))(m)
  m = tf.keras.layers.Dense(3, activation="softmax")(m)



  model = keras.Model(inputs=[input_tweets, input_target],outputs = m)

  
  
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer_adam(),
                metrics=['accuracy'])
  
  return model

