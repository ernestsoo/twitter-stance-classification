# -*- coding: utf-8 -*-
"""final_assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EHaZuvgenrh-qkdrVT1kLYww4jRJHa4r

# COSC 2779 Deep learning: Assignment 2
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

drive.mount('/content/gdrive/')

# %cd /content/gdrive/My Drive/StanceDataset/

# Check current OS directory
!ls

"""## Data Exploration & Analysis

### Data Retrieval
"""

# Import Essential dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import sklearn
import tensorflow as tf

!pip install nltk

import nltk

# Read train and test dataset.
with open('train.csv','r',encoding='ISO-8859-1') as f:
    train_data = pd.read_csv(f)

with open('test.csv','r',encoding='ISO-8859-1') as y:
    test_data = pd.read_csv(y)
    
    
f.close()
y.close()

train_data.head(20)

test_data.head(20)

train_data['Tweet']

# Number of Stance
train_data['Stance'].unique()

# Number of Target Topic
train_data['Target'].unique()

"""### Observe Stance Distribution for each target topic"""

import seaborn as sns

def plotTopicDist(TARGET_TOPIC):
  index = 0

  counter = [0,0,0]


  for topic in train_data['Target']:
    if topic == TARGET_TOPIC:
      y = 0
      for stance in train_data['Stance'].unique():
        if stance == train_data['Stance'][index]:
          counter[y] += 1
        y+=1


    index+=1

  df = pd.DataFrame([{'x':'AGAINST','y':counter[0]},{'x':'FAVOUR','y':counter[1]},{'x':'NONE','y':counter[2]}])
  sns.barplot(x = 'x',
              y = 'y',
              data = df)

  plt.title("TARGET TOPIC: "+str(TARGET_TOPIC))

  plt.show()

# Observe all target topic distributions
for topic in train_data['Target'].unique():
  plotTopicDist(topic)

"""### Visualizations of word distribution in Tweets"""

nltk.download('punkt')
nltk.download('stopwords')

"""### Word Cloud (Train Data)"""

!pip install wordcloud

from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator

def visualizeWordCloud(data):
  tweet_All = " ".join(review for review in data)

  plt.figure(figsize  = (30,30))
  # Create and generate a word cloud image:
  wordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(tweet_All)

  # Display the generated image:
  plt.imshow(wordcloud_ALL, interpolation='bilinear')
  plt.title('All Tweets', fontsize=30)
  plt.axis('off')

visualizeWordCloud(train_data['Tweet'])

"""### Word Cloud (Test Data)"""

visualizeWordCloud(test_data['Tweet'])

"""### Topic Specific WordCloud

**Hillary Clinton**
"""

def visualizeTopicWordCloud(TARGET_TOPIC):
  topicBool = train_data['Target'] == TARGET_TOPIC
  topic = train_data[topicBool]

  visualizeWordCloud(topic['Tweet'])

"""**Hillary Clinton**"""

visualizeTopicWordCloud("Hillary Clinton")

"""**Legalization of Abortion**"""

visualizeTopicWordCloud('Legalization of Abortion')

"""**Atheism**"""

visualizeTopicWordCloud('Atheism')

"""**Climate Change is a Real Concern**"""

visualizeTopicWordCloud('Climate Change is a Real Concern')

"""**Feminist Movement**"""

visualizeTopicWordCloud('Feminist Movement')

"""## Data Preparation

### Preparing Input & Target (x/y) values
"""

!pip install wordsegment

!pip install autocorrect

import string
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re
from wordsegment import load, segment
load()
from autocorrect import Speller

spell = Speller(lang='en')
tweets = list()
target = list()
hashtag = list()

#extracting hashtag keywords from tweet
train_data['hashtag'] = train_data['Tweet'].apply(lambda x: re.findall(r'\B#\w*[a-zA-Z]+\w*', x))

#extracting tagged users from tweet
train_data['users'] = train_data['Tweet'].apply(lambda x: re.findall(r'\B@\w*[a-zA-Z]+\w*', x))


lines_tweet = train_data["Tweet"].values.tolist()
lines_target = train_data["Target"].values.tolist()

for line in lines_tweet:


     
    #remove puntuations
    
    line = ' '.join(re.sub('([^0-9A-Za-z \t])|(\w+:\/\/\S+)', ' ', line).split())

    #line = ' '.join(segment(line))
    #line = ' '.join([spell(w) for w in line.split()])
    # tokenize the text
    tokens = word_tokenize(line)

    # convert to lower case
    tokens = [w.lower() for w in tokens]


    tweets.append(tokens)


for line in lines_target:
    
    tokens = word_tokenize(line)

    # convert to lower case
    tokens = [w.lower() for w in tokens]
    
    target.append(tokens)

# Max length for tweet sentences.
len(max(tweets,key = len))

# Max length for target sentences.
len(max(target,key = len))

from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

validation_split = 0.20
max_length_tweet = len(max(tweets,key = len))
max_length_target = len(max(target,key = len))

#max_length_hashtag = len(max(hashtag,key = len))

tokenizer = Tokenizer()
tokenizer.fit_on_texts(tweets)
#tokenizer.fit_on_texts(target)
#tokenizer.fit_on_texts(hashtag)


sequences_tweets = tokenizer.texts_to_sequences(tweets)
sequences_target = tokenizer.texts_to_sequences(target)

#sequences_hashtag = tokenizer.texts_to_sequences(hashtag)


word_index= tokenizer.word_index
print("unique tokens - "+str(len(word_index)))
vocab_size= len(tokenizer.word_index) + 1
print('vocab_size - '+str(vocab_size))

lines_pad_tweets = pad_sequences(sequences_tweets, maxlen=max_length_tweet, padding='post')
lines_pad_target = pad_sequences(sequences_target, maxlen=max_length_target, padding='post')
#lines_pad_hashtag = pad_sequences(sequences_hashtag, maxlen=max_length_hashtag, padding='post')


encode = {
    "AGAINST" : 0,
    "NONE"  : 1,       
    "FAVOR" : 2
}

train_data["Stance"] = train_data["Stance"].apply(lambda x: encode[x])

category = train_data['Stance'].values

indices = np.arange(lines_pad_tweets.shape[0])
np.random.shuffle(indices)
lines_pad_tweets = lines_pad_tweets[indices]
lines_pad_target = lines_pad_target[indices]
#lines_pad_hashtag = lines_pad_hashtag[indices]
category = category[indices]

n_values = np.max(category) + 1
Y = np.eye(n_values)[category]

num_validation_samples = int(validation_split * lines_pad_tweets.shape[0])

X_train_tweets_pad = lines_pad_tweets[:-num_validation_samples]
X_train_target_pad = lines_pad_target[:-num_validation_samples]
#X_train_hashtag_pad = lines_pad_hashtag[:-num_validation_samples]
y_train = Y[:-num_validation_samples]
X_val_tweets_pad = lines_pad_tweets[-num_validation_samples:]
X_val_target_pad = lines_pad_target[-num_validation_samples:]
#X_val_hashtag_pad = lines_pad_hashtag[-num_validation_samples:]
y_val = Y[-num_validation_samples:]

print('Shape of X_train_tweet_pad:', X_train_tweets_pad.shape)
print('Shape of X_train_target_pad:', X_train_target_pad.shape)
#print('Shape of X_train_hashtag_pad:', X_train_hashtag_pad.shape)
print('Shape of y_train:', y_train.shape)

print('Shape of X_test_tweet_pad:', X_val_tweets_pad.shape)
print('Shape of X_test_target_pad:', X_val_target_pad.shape)
#print('Shape of X_test_hashtag_pad:', X_val_hashtag_pad.shape)
print('Shape of y_test:', y_val.shape)

X_train_tweets_pad

X_val_target_pad

"""### Preparing Test Dataset"""

# Removing Donald Trump Target Topic
test_data = test_data[test_data['Target'] != "Donald Trump"]

spell = Speller(lang='en')
test_tweets = list()
test_target = list()
test_hashtag = list()

#extracting hashtag keywords from tweet
test_data['hashtag'] = test_data['Tweet'].apply(lambda x: re.findall(r'\B#\w*[a-zA-Z]+\w*', x))

#extracting tagged users from tweet
test_data['users'] = test_data['Tweet'].apply(lambda x: re.findall(r'\B@\w*[a-zA-Z]+\w*', x))


lines_tweet = test_data["Tweet"].values.tolist()
lines_target = test_data["Target"].values.tolist()

for line in lines_tweet:
    #remove puntuations
    
    line = ' '.join(re.sub('([^0-9A-Za-z \t])|(\w+:\/\/\S+)', ' ', line).split())
    
    # tokenize the text
    tokens = word_tokenize(line)

    # convert to lower case
    tokens = [w.lower() for w in tokens]

    test_tweets.append(tokens)


for line in lines_target:
    
    tokens = word_tokenize(line)

    # convert to lower case
    tokens = [w.lower() for w in tokens]
    
    test_target.append(tokens)

test_lines_hashtag = test_data["hashtag"].values.tolist()


for line in test_lines_hashtag:
  line = ' '.join(line)
  line = ' '.join(re.sub('([^0-9A-Za-z \t])|(\w+:\/\/\S+)', ' ', line).split())
  line = ' '.join(segment(line))
  tokens = word_tokenize(line)

  # convert to lower case
  tokens = [w.lower() for w in tokens]
    
  test_hashtag.append(tokens)

test_sequences_tweets = tokenizer.texts_to_sequences(test_tweets)
test_sequences_target = tokenizer.texts_to_sequences(test_target)
#test_sequences_hashtag = tokenizer.texts_to_sequences(test_hashtag)

test_lines_pad_tweets = pad_sequences(test_sequences_tweets, maxlen=max_length_tweet, padding='post')
test_lines_pad_target = pad_sequences(test_sequences_target, maxlen=max_length_target, padding='post')
#test_lines_pad_hashtag = pad_sequences(test_sequences_hashtag, maxlen=max_length_hashtag, padding='post')


encode = {
    "AGAINST" : 0,
    "NONE"  : 1,       
    "FAVOR" : 2
}

test_data["Stance"] = test_data["Stance"].apply(lambda x: encode[x])

category = test_data['Stance'].values

print("Test Tweets Shape", test_lines_pad_tweets.shape)
print("Test Targets Shape",test_lines_pad_target.shape)
print("Test Stance Shape",test_data['Stance'].shape)

from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score
    
def getTestAccuracy(model):
    y_pred = model.predict([ test_lines_pad_tweets, test_lines_pad_target])
    
    y_pred_max = np.argmax(y_pred, axis=1)
    
    return str("{:.2f}".format(accuracy_score(test_data['Stance'], y_pred_max) * 100))+"%"

def getF1ScoreMacro(model):
    y_pred = model.predict([ test_lines_pad_tweets, test_lines_pad_target])
    
    y_pred_max = np.argmax(y_pred, axis=1)

    return str("{:.2f}".format(f1_score(test_data['Stance'], y_pred_max, average="macro")* 100))+"%"

def getF1ScoreMicro(model):
    y_pred = model.predict([ test_lines_pad_tweets, test_lines_pad_target])
    
    y_pred_max = np.argmax(y_pred, axis=1)

    return str("{:.2f}".format(f1_score(test_data['Stance'], y_pred_max, average="micro")* 100))+"%"

def getF1ScoreAverage(model):
    y_pred = model.predict([ test_lines_pad_tweets, test_lines_pad_target])
    
    y_pred_max = np.argmax(y_pred, axis=1)

    return str("{:.2f}".format(((f1_score(test_data['Stance'], y_pred_max, average="macro")+f1_score(test_data['Stance'], y_pred_max, average="micro"))/2)* 100))+"%"

def getPerformanceReport(model):

    performance = []

    y_pred = model.predict([test_lines_pad_tweets, test_lines_pad_target])
    
    y_pred_max = np.argmax(y_pred, axis=1)

    f1_macro = str("{:.2f}".format(f1_score(test_data['Stance'], y_pred_max, average="macro")* 100))+"%"
    f1_micro = str("{:.2f}".format(f1_score(test_data['Stance'], y_pred_max, average="micro")* 100))+"%"
    f1_average = str("{:.2f}".format(((f1_score(test_data['Stance'], y_pred_max, average="macro")+f1_score(test_data['Stance'], y_pred_max, average="micro"))/2)* 100))+"%"

    performance.append({"Target": "All", "F1-Micro": f1_micro, "F1-Macro": f1_macro, "F1-Average":f1_average})
    for target in test_data["Target"].unique():
      index = 0

      y_test_target = []
      y_pred_target = []
      
      for test_target in test_data['Target']:
        if target == test_target:
          y_test_target.append(test_data["Stance"][index])
          y_pred_target.append(y_pred_max[index])

        index+=1

      f1_macro = str("{:.2f}".format(f1_score(y_test_target, y_pred_target, average="macro")* 100))+"%"
      f1_micro = str("{:.2f}".format(f1_score(y_test_target, y_pred_target, average="micro")* 100))+"%"
      f1_average = str("{:.2f}".format(((f1_score(y_test_target, y_pred_target, average="macro")+f1_score(y_test_target, y_pred_target, average="micro"))/2)* 100))+"%"

      performance.append({"Target": target, "F1-Micro": f1_micro, "F1-Macro": f1_macro, "F1-Average":f1_average})

    return pd.DataFrame(performance)

import seaborn as sns 

def getPredictionBarPlot(model, target):
  y_pred = model.predict([ test_lines_pad_tweets, test_lines_pad_target])
    
  y_pred_max = np.argmax(y_pred, axis=1)

  index = 0
  rows_list = []

  counter = [0,0,0,0,0,0]

  for val in test_data["Target"]:
    if val == target:
      if y_pred_max[index] == 0:
        counter[0]+= 1
      elif y_pred_max[index] == 1:
        counter[1] += 1
      elif y_pred_max[index] == 2:
        counter[2] += 1
        
      if test_data['Stance'][index] == 0:
        counter[3] += 1
      elif test_data['Stance'][index] == 1:
        counter[4] += 1
      elif test_data['Stance'][index] == 2:
        counter[5] += 1
  
    index += 1

  rows_list.append({"type": "actual", "class": "favor", "count": counter[0]})
  rows_list.append({"type": "actual", "class": "neutral", "count": counter[1]})
  rows_list.append({"type": "actual", "class": "against", "count": counter[2]})
  rows_list.append({"type": "predicted", "class": "favor", "count": counter[3]})
  rows_list.append({"type": "predicted", "class": "neutral", "count": counter[4]})
  rows_list.append({"type": "predicted", "class": "against", "count": counter[5]})

  stats_df = pd.DataFrame(rows_list)  

  sns.barplot(x="class", y="count", hue="type", data=stats_df)
  plt.ylabel("Count", size=14)
  plt.xlabel("Class", size=14)
  plt.title("Predicted vs Actual: " + target, size=18)

"""### Importing GloVe Twitter Embedding Vector"""

# Check file directory.
!ls

# Import GloVe word embeddings train on text corpus of 27 billion tweets.
file = open('glove.twitter.27B.200d.txt', encoding='utf-8')

glove_vectors = dict()
for line in file:
  values = line.split()
  word = values[0]
  features = np.asarray(values[1:])
  glove_vectors[word] = features

file.close()

# Initialze embedding dim.
embedding_dim = 200

# Map GloVe emebddings to current vocabulary
E_T_200d= np.zeros((len(word_index) + 1, embedding_dim))
for word, i in word_index.items():
    embedding_vector = glove_vectors.get(word)
    if embedding_vector is not None:
        E_T_200d[i] = embedding_vector

# Delete Vector dictionary to save memory on instance.
del glove_vectors

# Check Embedding Vector Shape
print("200d vector shape:", E_T_200d.shape)

"""### Prepare Visualization Tools"""

from itertools import cycle

# Compare plots for different models.
def plotter(history_hold, metric = 'loss', ylim=[0.0, 1.0]):
  cycol = cycle('bgrcmk')
  for name, item in history_hold.items():
    y_train = item.history[metric]
    y_val = item.history['val_' + metric]
    x_train = np.arange(0,len(y_val))

    c=next(cycol)

    plt.plot(x_train, y_train, c+'-', label=name+'_train')
    plt.plot(x_train, y_val, c+'--', label=name+'_val')

  plt.legend()
  plt.xlim([1, max(plt.xlim())])
  plt.ylim(ylim)
  plt.xlabel('Epoch')
  plt.ylabel(metric)
  plt.grid(True)

# plot the evolution of Loss and Acuracy on the train and validation sets
import matplotlib.pyplot as plt

def plotter_single(history):
    plt.figure(figsize=(20,10))
    plt.subplot(1, 2, 1)
    plt.suptitle('Optimizer : Adam', fontsize=10)
    plt.ylabel('Loss', fontsize=16)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.legend(loc='upper right')

    plt.subplot(1, 2, 2)
    plt.ylabel('Accuracy', fontsize=16)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.show()

# Compare training and val loss or accuracy on last epoch.
def compare_params(metric="loss", h_histories= {}, params_list=[], param_name="Lambda", model_name="model_"):
  plt.figure(figsize=(10,5))
  l_train = list()
  l_val = list()

  for param in params_list:
    l_train.append(h_histories[model_name+ '_h' + str(param)].history[metric][-1])
    l_val.append(h_histories[model_name+ '_h' + str(param)].history['val_' + metric][-1])

  plt.plot(params_list,l_train, 'ro', label='Train' )
  plt.plot(params_list,l_val, 'bs', label='Test' )

  plt.xlabel(param_name, fontsize=14)
  plt.ylabel(metric, fontsize=14)
  plt.legend()
  plt.show()

"""## Selected Model Architecture"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense,Dropout,Embedding,Bidirectional,Input, Multiply, Concatenate

from tensorflow.keras import regularizers
from tensorflow.keras import Model
from tensorflow.keras import optimizers

from tensorflow.keras.models import Sequential

import tensorflow.keras.backend as K

from tensorflow.keras import initializers
from tensorflow.keras.layers import BatchNormalization

"""Attention layer source: https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/"""

from keras.layers import Layer
import keras.backend as K

class attention(Layer):
    def __init__(self,**kwargs):
        super(attention,self).__init__(**kwargs)

    def build(self,input_shape):
        self.W=self.add_weight(name="att_weight",shape=(input_shape[-1],1),initializer="normal")
        self.b=self.add_weight(name="att_bias",shape=(input_shape[1],1),initializer="zeros")        
        super(attention, self).build(input_shape)

    def call(self,x):
        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)
        at=K.softmax(et)
        at=K.expand_dims(at,axis=-1)
        output=x*at
        return K.sum(output,axis=1)

    def compute_output_shape(self,input_shape):
        return (input_shape[0],input_shape[-1])

    def get_config(self):
        return super(attention,self).get_config()

def optimizer_adam(dec_rate = 30,lr = 0.001, xtrain = None, batch_size = 32):
  STEPS_PER_EPOCH = xtrain.shape[0] // batch_size
  lr_adam = lr

  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  lr_adam,
  decay_steps=STEPS_PER_EPOCH*1000,
  decay_rate=dec_rate,
  staircase=False)
  opt_adam = optimizers.Adam(learning_rate=lr_schedule)
  return opt_adam

# Model tweets conditioned on Target.
def model_conditional_encoding(embedding_dim = 200, dropout_rate = 0.45, lambda_val = 0.0025, learning_rate=0.0004, batch_size = 32, E_T= E_T_200d):
  
  ##########################
  # Model Inputs
  ##########################

  # define three sets of inputs (Tweets, Target Topic, Hashtag)
  input_tweets = Input(shape=(X_train_tweets_pad.shape[1],))
  input_target = Input(shape=(X_train_target_pad.shape[1],))
  
  input_tweet_length = X_train_tweets_pad.shape[1]
  input_target_length = X_train_target_pad.shape[1]

  # Initalize Embedding Paramaters
  word_embedding_dim = embedding_dim
  

  # Number of Neurons in RNN Layer.
  x_RNN_dim = 128
  y_RNN_dim = 128
  

  ##########################
  # First Branch (Tweet)
  ##########################

  # the first branch (model x), that operates on the first input (input target)
  x = Embedding(output_dim=word_embedding_dim,
                      input_dim=vocab_size,
                      input_length=max_length_target,
                      weights=[E_T],
                      trainable=False)(input_target)


  x = Dropout(dropout_rate)(x)


  target_encoding = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x_RNN_dim,  
                                    return_sequences=False, 
                                    return_state = True,
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))

  (target_encoding, target_forward_h, target_forward_s,target_backward_h, target_backward_s) = target_encoding(x)



  ##########################
  # Second Branch (Tweet)
  # Initalize Second Branch with First Branch encoding.
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  y = Embedding(output_dim=word_embedding_dim,
                      input_dim=vocab_size,
                      input_length=max_length_tweet,
                      weights=[E_T],
                      trainable=False)(input_tweets)

  y = Dropout(dropout_rate)(y)

  tweet_encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y_RNN_dim,  
                                    return_sequences=True,
                                    return_state = False, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))


  tweet_encoder_out = tweet_encoder(y, initial_state=[target_forward_h, target_forward_s,target_backward_h, target_backward_s])


  ##########################
  # Attention Layer
  ##########################
  attention_out = attention()(tweet_encoder_out) 


  ##########################
  # FC and Final Layer
  ##########################

  FC_dim_1 = 256
  FC_dim_2 = 256
  nb_classes = 3

  # apply two FC layers and then a softmax layer to predict the stance classes.
  z = Dense(FC_dim_1, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(attention_out)

  z = Dropout(dropout_rate)(z)

  z = Dense(FC_dim_2, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(z)

  z = Dropout(dropout_rate)(z)

  z = BatchNormalization()(z)
  
  z = Dense(nb_classes, activation="softmax")(z)



  ##########################
  # Model Building
  ##########################

  # Build combined model.
  model = Model(inputs=[input_tweets,input_target], outputs=z)

  
  ##########################
  # Model Compilation
  ##########################

  # Compile model
  #lr = learning_rate
  optimizer = optimizer_adam(lr=learning_rate, xtrain=X_train_tweets_pad, batch_size = batch_size )
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])
  
  return model

"""### Model Summary"""

model_conditional_encoding().summary()

"""## Hyperparameter Search

In this section, a grid search approach will be used to find the most optimal hyperparameters for the best model architecture designed in this notebook:

The hyperparameters that will be searched are (in order of importance):


*   Learning Rate (alpha)
*   Dropout Rate
*   L2 Regularization (lambda)
*   Batch Size

### Learning Rate
"""

h_alpha_histories = {}

# Learning Rate Values (Previously determined desired range).
alpha_vals = [0.001, 0.0005, 0.00025, 0.0001]

# Perform grid search.
for val in alpha_vals:

  # Define Fitting Parameters
  VAL_SPLIT = 0.2
  MAX_EPOCHS = 80
  BATCH_SIZE = 64
  VERBOSE = 1
  SHUFFLE = True

  # Define model
  h_model = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d, learning_rate = val, batch_size=BATCH_SIZE)

  h_alpha_histories['model_ce_alpha'+ '_h' + str(val)] = h_model.fit(
                                x=[X_train_tweets_pad,X_train_target_pad],
                                y=y_train,
                                validation_data=([X_val_tweets_pad,X_val_target_pad],y_val),
                                epochs=MAX_EPOCHS,
                                batch_size=BATCH_SIZE,
                                verbose=VERBOSE,
                                shuffle=SHUFFLE)

plotter(h_alpha_histories, ylim=[0.5,3])

plotter(h_alpha_histories, metric="accuracy", ylim=[0.4,0.8])

compare_params(params_list=alpha_vals,h_histories=h_alpha_histories, param_name="Alpha", model_name="model_ce_alpha")

compare_params(metric="accuracy",h_histories=h_alpha_histories, params_list=alpha_vals, param_name="Alpha", model_name="model_ce_alpha")

"""### Dropout Rate"""

h_dropout_histories = {}

# Dropout values.
dropout_vals = [0.4, 0.45, 0.5, 0.55]

best_learning_rate = 0.0005

# Perform grid search.
for val in dropout_vals:

  # Define Fitting Parameters
  VAL_SPLIT = 0.2
  MAX_EPOCHS = 80
  BATCH_SIZE = 64
  VERBOSE = 1
  SHUFFLE = True

  h_model = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d, learning_rate = best_learning_rate, dropout_rate = val, batch_size = BATCH_SIZE )

  

  h_dropout_histories['model_ce_dropout'+ '_h' + str(val)] = h_model.fit(
                                x=[X_train_tweets_pad,X_train_target_pad],
                                y=y_train,
                                validation_data=([X_val_tweets_pad,X_val_target_pad],y_val),
                                epochs=MAX_EPOCHS,
                                batch_size=BATCH_SIZE,
                                verbose=VERBOSE,
                                shuffle=SHUFFLE)

plotter(h_dropout_histories, ylim=[0.5,3])

plotter(h_dropout_histories,metric="accuracy", ylim=[0.5,0.8])

compare_params(params_list=dropout_vals, h_histories=h_dropout_histories,param_name="Dropout Rate", model_name="model_ce_dropout")

compare_params(params_list=dropout_vals, h_histories=h_dropout_histories,param_name="Dropout Rate", model_name="model_ce_dropout", metric="accuracy")

"""### L2 Regularization"""

h_lambda_histories = {}

# L2 regularization values.
lambda_vals = [0.001, 0.0025, 0.005, 0.01]

best_learning_rate = 0.0005
best_dropout_rate = 0.5

# Perform grid search.
for val in lambda_vals:
  # Define Fitting Parameters
  VAL_SPLIT = 0.2
  MAX_EPOCHS = 80
  BATCH_SIZE = 64
  VERBOSE = 1
  SHUFFLE = True

  h_model = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d, learning_rate = best_learning_rate, dropout_rate=best_dropout_rate, lambda_val = val, batch_size = BATCH_SIZE)

  h_lambda_histories['model_ce_lambda'+ '_h' + str(val)] = h_model.fit(
                                x=[X_train_tweets_pad,X_train_target_pad],
                                y=y_train,
                                validation_data=([X_val_tweets_pad,X_val_target_pad],y_val),
                                epochs=MAX_EPOCHS,
                                batch_size=BATCH_SIZE,
                                verbose=VERBOSE,
                                shuffle=SHUFFLE)

plotter(h_lambda_histories, ylim=[0.5,3])

plotter(h_lambda_histories,metric="accuracy", ylim=[0.5,0.8])

compare_params(params_list=lambda_vals, h_histories=h_lambda_histories,param_name="Lambda Val", model_name="model_ce_lambda")

compare_params(params_list=lambda_vals, h_histories=h_lambda_histories,param_name="Lambda Val", model_name="model_ce_lambda", metric="accuracy")

"""### Batch Size"""

h_batch_histories = {}

# Learning Rate Values.
batch_vals = [16, 32, 64]

best_learning_rate = 0.0005
best_dropout_rate = 0.5
best_lambda_val = 0.0025

# Perform grid search.
for val in batch_vals:

  # Define Fitting Parameters
  VAL_SPLIT = 0.2

  MAX_EPOCHS = 50
  BATCH_SIZE = val
  VERBOSE = 1
  SHUFFLE = True

  h_model = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d, learning_rate = best_learning_rate, dropout_rate= best_dropout_rate, lambda_val= best_lambda_val, batch_size = BATCH_SIZE )

  h_batch_histories['model_ce_batch'+ '_h' + str(val)] = h_model.fit(
                                x=[X_train_tweets_pad,X_train_target_pad],
                                y=y_train,
                                validation_data=([X_val_tweets_pad,X_val_target_pad],y_val),
                                epochs=MAX_EPOCHS,
                                batch_size=BATCH_SIZE,
                                verbose=VERBOSE,
                                shuffle=SHUFFLE)

plotter(h_batch_histories, ylim=[0.5,3])

plotter(h_batch_histories, metric="accuracy", ylim=[0.5,0.8])

compare_params(params_list=batch_vals, h_histories=h_batch_histories,param_name="Batch Size", model_name="model_ce_batch")

compare_params(params_list=batch_vals, h_histories=h_batch_histories,param_name="Batch Size", model_name="model_ce_batch", metric="accuracy")

"""## Final Model

### Training of final model with selected hyper-parameters
"""

# Define best hyperparameters detemined from grid search above.
best_learning_rate = 0.0005
best_dropout_rate = 0.5
best_lambda_val = 0.0025
best_batch_size = 64


# Define Fitting Parameters
MAX_EPOCHS = 200
BATCH_SIZE = best_batch_size
VERBOSE = 1
SHUFFLE = True

# Defined final model architecture
final_model = model_conditional_encoding(embedding_dim=200, learning_rate = best_learning_rate, dropout_rate = best_dropout_rate, lambda_val=best_lambda_val, batch_size = BATCH_SIZE)

# Early stopping with patience of 10 epochs.
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=VERBOSE)

CALLBACKS = [early_stopping]


history_final_model = final_model.fit(
                  x=[X_train_tweets_pad,X_train_target_pad],
                  y=y_train,
                  validation_data=([X_val_tweets_pad,X_val_target_pad],y_val),
                  epochs=MAX_EPOCHS,
                  batch_size=BATCH_SIZE,
                  verbose=VERBOSE,
                  shuffle=SHUFFLE,
                  callbacks=CALLBACKS)

plotter_single(history_final_model)

histories = {}

histories["final_model"] = history_final_model

plotter(histories, ylim = [0.5, 2])

plotter(histories, metric="accuracy", ylim = [0.5, 0.8])

"""### Prediction on Unseen Data"""

getF1ScoreMicro(final_model)

getF1ScoreMacro(final_model)

getF1ScoreAverage(final_model)

getPerformanceReport(final_model)

uniqueTarget = test_data["Target"].unique()

getPredictionBarPlot(final_model,uniqueTarget[0])

getPredictionBarPlot(final_model,uniqueTarget[1])

getPredictionBarPlot(final_model, uniqueTarget[2])

getPredictionBarPlot(final_model, uniqueTarget[3])

getPredictionBarPlot(final_model, uniqueTarget[4])

"""### Independant Evaluation"""

!pip install twython

from twython import Twython
APP_KEY = '28ZLoVRN8SE1aoA1hxU4wjgvY'
APP_SECRET = '0KmYLLmkzQy1cFmhXqTuBwCFe7YTB869qy0mdztHd85zrTleYn'

twitter = Twython(APP_KEY, APP_SECRET)
search_results = twitter.search(q="#GoHillary", rpp="40")

print(search_results)

for tweet in search_results["statuses"]:
    # print("Tweet from @%s Date: %s" % (tweet['from_user'].encode('utf-8'),tweet['created_at']))
    print(tweet['text'])

# Save Tweets: #prayToEnd
tweets_found = []

##########################
# Hillary Clinton: Favor
##########################

# https://twitter.com/VLHScan/status/931887685471481857
tweets_found.append({"tweet": "Hillary Clinton had to battle the GOP, FBI & Russia....AND still managed to kick your ass in the popular vote by millions.", "target": "Hillary Clinton", "stance": "Favor", "link": "https://twitter.com/VLHScan/status/931887685471481857"})

# https://twitter.com/goodtroubleme/status/1269510787552305152
tweets_found.append({"tweet": "While Hillary endangered her life by going undercover in the early 70s to expose racism in Southern schools, Bernie, a grown man was writing how women fantasize about being raped by three men. Thank God, nobody said #STFUHillary back then! #GoHillary!", "target": "Hillary Clinton", "stance": "Favor", "link": "https://twitter.com/goodtroubleme/status/1269510787552305152"})

# https://twitter.com/barbls23/status/1132373945674547200
tweets_found.append({"tweet": "#GoHillary Fighting to save our country from a wanna be Autocrat @realDonaldTrump and his lapdog Republican, who are selling out America to preserve their power! They are a disgrace to America and its people! #IWillAlwaysBeWithHillary!", "target": "Hillary Clinton", "stance": "Favor", "link": "https://twitter.com/barbls23/status/1132373945674547200"})

# https://twitter.com/TrotDarrow/status/907647225714454528
tweets_found.append({"tweet": "Good for you, #Hillary, to remind that you have the experience, the scars, etc. to speak out & that IS exactly what you must do. #GoHillary", "target": "Hillary Clinton", "stance": "Favor", "link": "https://twitter.com/TrotDarrow/status/907647225714454528"})

# https://twitter.com/FPly/status/964654963384176640
tweets_found.append({"tweet": "She should be in office right now. Why does a woman always have to come along and clean up a man's mess? And what a colossal mess this tRump has made! #GoHillary!", "target": "Hillary Clinton", "stance": "Favor", "link": "https://twitter.com/FPly/status/964654963384176640"})



####################################
# Legalization of Abortion: Against
####################################

# https://twitter.com/FPly/status/964654963384176640
tweets_found.append({"tweet": "Lord Jesus Christ, we pray for your precious, unborn babies and an end to abortion. #prolife #PrayToEndAbortion", "target": "Legalization of Abortion", "stance": "Against", "link": "https://twitter.com/roseOyuma2ndAcc/status/1312207885066145792"})

# https://twitter.com/damarispatroci/status/1313308107368472576
tweets_found.append({"tweet": "It's Monday...and that means it's time to update the number of babies saved from abortion through your prayers. Today, we know of 102 babies spared from abortion since September 23. Thanks be to God! #babysaved #praytoendabortion #40daysforlife #godisgood", "target": "Legalization of Abortion", "stance": "Against", "link": "https://twitter.com/damarispatroci/status/1313308107368472576"})

# https://twitter.com/nculturae/status/1314387731594661890
tweets_found.append({"tweet": "Proud Boys showed up at my local Planned Parenthood.  My group prayed to end abortion while theirs waved their flags and got a lot of positive attention. Really good guys. #40daysforlife #praytoendabortion", "target": "Legalization of Abortion", "stance": "Against", "link": "https://twitter.com/nculturae/status/1314387731594661890"})

# https://twitter.com/BishopStika/status/1311862408034611201
tweets_found.append({"tweet": "One should never assume anything about the private life of a person. One can however make judgements about political people who say that they are faithful Catholics and yet allow the slaughter of the most innocent. Abortion is the ultimate child abuse.", "target": "Legalization of Abortion", "stance": "Against", "link": "https://twitter.com/BishopStika/status/1311862408034611201"})

# https://twitter.com/HelpersMichigan/status/1312371331439230977
tweets_found.append({"tweet": "May God have mercy upon us and may we soon see the day this country ends the murder of infants. #PrayToEndAbortion", "target": "Legalization of Abortion", "stance": "Against", "link": "https://twitter.com/HelpersMichigan/status/1312371331439230977"})


####################################
# Legalization of Abortion: Favor
####################################

# https://twitter.com/kerrywashington/status/1314025193870774273
tweets_found.append({"tweet": "I am #prochoice. It’s my body. It’s my life. @VP You think Americans have a right to make informed decisions about their masks? I think Americans have a right to make a decision about our uteruses. #VPDebate", "target": "Legalization of Abortion", "stance": "Favor", "link": "https://twitter.com/kerrywashington/status/1314025193870774273"})

# https://twitter.com/prochoiceforal1/status/1313462177190338560
tweets_found.append({"tweet": "The same people who are against abortion are the same people who want to overturn marriage equality. Why? Because they don’t care about life, they just want to enforce their extremists religious ideals onto everyone. #prochoice #prolife", "target": "Legalization of Abortion", "stance": "Favor", "link": "https://twitter.com/prochoiceforal1/status/1313462177190338560"})

# https://twitter.com/OptixWasTaken/status/1313151934228918277
tweets_found.append({"tweet": "Pro choice aint forcing abortions on anyone; its offering safe procedures to have them if chosen. I'd rather a woman be able to have the abortion through safe manners than having to risk injury or even death to themselves because they had to go through old methods! #prochoice", "target": "Legalization of Abortion", "stance": "Favor", "link": "https://twitter.com/OptixWasTaken/status/1313151934228918277"})

# https://twitter.com/elzey_t/status/1313128445732364289
tweets_found.append({"tweet": "#ProChoice You don’t get to decide a medical procedure for women. It is completely the responsibility of her and her physician. It’s not the government or anyone else’s business.", "target": "Legalization of Abortion", "stance": "Favor", "link": "https://twitter.com/elzey_t/status/1313128445732364289"})

# https://twitter.com/CSCO303/status/1314053516915208192
tweets_found.append({"tweet": "Religious or personal beliefs on   #abortion are your own & you don’t get to dictate to the rest of us what we should believe & doesn’t have any bearing on my medical choices. Just like you can’t be forcibly sterilized. You do you leave my uterus to me #ProChoice #VoteBidenHarris", "target": "Legalization of Abortion", "stance": "Favor", "link": "https://twitter.com/CSCO303/status/1314053516915208192"})



####################################
# Atheism: Favor
####################################

# https://twitter.com/rogerlsmith/status/1036092318967844864
tweets_found.append({"tweet": "With news about the pope this week, Christianity is and always has been, simple hypocrisy.  #NoMoreReligion", "target": "Atheism", "stance": "Favor", "link": "https://twitter.com/rogerlsmith/status/1036092318967844864"})

# https://twitter.com/seanfinan8/status/866910547249418240
tweets_found.append({"tweet": "People being murdered for years over who has the better imaginery friend!! It has to stop!! #nomorereligion", "target": "Atheism", "stance": "Favor", "link": "https://twitter.com/seanfinan8/status/866910547249418240"})

# https://twitter.com/Atheist_Trooper/status/1314770190136225794
tweets_found.append({"tweet": "Religion lives off from fear and division among the people. Any logical person can clearly see this truth - but the real consequence of this is that its toxicity has spread into politics. And this truth is honestly terrifying. Keep them separated, period. #Atheist #Atheism", "target": "Atheism", "stance": "Favor", "link": "https://twitter.com/Atheist_Trooper/status/1314770190136225794"})

# https://twitter.com/NicolaMiddleton/status/1106948723844833280
tweets_found.append({"tweet": "We have to shift our focus away from religion to the spiritual mystery we are in and to becoming love. #NewFocus #NewParadigm #NoMoreReligion #SpiritualMystery #BecomingLove", "target": "Atheism", "stance": "Favor", "link": "https://twitter.com/NicolaMiddleton/status/1106948723844833280"})

# https://twitter.com/AntEyeTheist/status/1314550939508932610
tweets_found.append({"tweet": "God is about death denial. We are the only animal on earth that knows we will some day die.  It could happen at any moment and that is psychologically terrifying.  We will never be free until we recon with death, the mainspring of all human activity.  #WormAtTheCore #Atheism", "target": "Atheism", "stance": "Favor", "link": "https://twitter.com/AntEyeTheist/status/1314550939508932610"})






independant_tweets = pd.DataFrame(tweets_found)

independant_tweets

import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re
from wordsegment import load, segment
load()
from autocorrect import Speller

spell = Speller(lang='en')
ind_tweets = list()
ind_target = list()
hashtag = list()



lines_tweet = independant_tweets["tweet"].values.tolist()
lines_target = independant_tweets["target"].values.tolist()

for line in lines_tweet:

    #remove puntuations  
    line = ' '.join(re.sub('([^0-9A-Za-z \t])|(\w+:\/\/\S+)', ' ', line).split())

    # tokenize the text
    tokens = word_tokenize(line)

    # convert to lower case
    tokens = [w.lower() for w in tokens]

    ind_tweets.append(tokens)


for line in lines_target:
    
    tokens = word_tokenize(line)

    # convert to lower case
    tokens = [w.lower() for w in tokens]
    
    ind_target.append(tokens)

ind_sequences_tweets = tokenizer.texts_to_sequences(ind_tweets)
ind_sequences_target = tokenizer.texts_to_sequences(ind_target)

ind_lines_pad_tweets = pad_sequences(ind_sequences_tweets, maxlen=max_length_tweet, padding='post')
ind_lines_pad_target = pad_sequences(ind_sequences_target, maxlen=max_length_target, padding='post')


encode = {
    "Against" : 0,
    "None"  : 1,       
    "Favor" : 2
}

independant_tweets["stance"] = independant_tweets["stance"].apply(lambda x: encode[x])

y_pred = final_model.predict([ ind_lines_pad_tweets, ind_lines_pad_target])
    
y_pred_max = np.argmax(y_pred, axis=1)

# Micro F1-score  
str("{:.2f}".format(f1_score(independant_tweets['stance'], y_pred_max, average="micro") * 100))+"%"

# Macro F1-score
str("{:.2f}".format(f1_score(independant_tweets['stance'], y_pred_max, average="macro") * 100))+"%"

# Average F1-Score
str("{:.2f}".format(((f1_score(independant_tweets['stance'], y_pred_max, average="macro")+f1_score(independant_tweets['stance'], y_pred_max, average="micro"))/2)* 100))+"%"

def decode_pred(index):
  if index ==0:
    return "Against"
  elif index == 1:
    return "None"
  elif index == 2:
    return "Favor"

decode = {
    0: "Against",
    1: "None",       
    2: "Favor"
}

independant_tweets['stance'] = independant_tweets['stance'].apply(lambda x: decode[x])
#y_pred_max = y_pred_max.apply(lambda x: encode[x])
for index, row in independant_tweets.iterrows():
  print("Tweet: " + row['tweet'])
  print("\n")
  print("Actual: " + row['stance'])
  print("Predicted: " + decode_pred(y_pred_max[index]) )
  print("\n\n\n__________________________________\n")