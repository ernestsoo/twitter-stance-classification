# -*- coding: utf-8 -*-
"""assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dMW8Eb2dVlAJu-36UZyiK1koHpLTkp-U

# COSC 2779 Deep learning: Assignment 2
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

drive.mount('/content/gdrive/')

# %cd /content/gdrive/My Drive/StanceDataset/

# Check current OS directory
!ls

"""## Data Exploration & Analysis

### Data Retrieval
"""

# Import Essential dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import sklearn
import tensorflow as tf

!pip install nltk

import nltk

# Read train and test dataset.
with open('train_shuffled.csv','r',encoding='ISO-8859-1') as f:
    train_data = pd.read_csv(f)

with open('test.csv','r',encoding='ISO-8859-1') as y:
    test_data = pd.read_csv(y)
    
    
f.close()
y.close()

train_data.head(20)

test_data.head(20)

train_data['Tweet']

# Number of Stance
train_data['Stance'].unique()

# Number of Target Topic
train_data['Target'].unique()

"""### Observe Stance Distribution for each target topic"""

import seaborn as sns

def plotTopicDist(TARGET_TOPIC):
  index = 0

  counter = [0,0,0]


  for topic in train_data['Target']:
    if topic == TARGET_TOPIC:
      y = 0
      for stance in train_data['Stance'].unique():
        if stance == train_data['Stance'][index]:
          counter[y] += 1
        y+=1


    index+=1

  df = pd.DataFrame([{'x':'AGAINST','y':counter[0]},{'x':'FAVOUR','y':counter[1]},{'x':'NONE','y':counter[2]}])
  sns.barplot(x = 'x',
              y = 'y',
              data = df)

  plt.title("TARGET TOPIC: "+str(TARGET_TOPIC))

  plt.show()

# Observe all target topic distributions
for topic in train_data['Target'].unique():
  plotTopicDist(topic)

"""### Visualizations of word distribution in Tweets"""

nltk.download('punkt')
nltk.download('stopwords')

"""### Word Cloud (Train Data)"""

!pip install wordcloud

from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator

def visualizeWordCloud(data):
  tweet_All = " ".join(review for review in data)

  plt.figure(figsize  = (30,30))
  # Create and generate a word cloud image:
  wordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(tweet_All)

  # Display the generated image:
  plt.imshow(wordcloud_ALL, interpolation='bilinear')
  plt.title('All Tweets', fontsize=30)
  plt.axis('off')

visualizeWordCloud(train_data['Tweet'])

"""### Word Cloud (Test Data)"""

visualizeWordCloud(test_data['Tweet'])

"""### Topic Specific WordCloud

**Hillary Clinton**
"""

def visualizeTopicWordCloud(TARGET_TOPIC):
  topicBool = train_data['Target'] == TARGET_TOPIC
  topic = train_data[topicBool]

  visualizeWordCloud(topic['Tweet'])

"""**Hillary Clinton**"""

visualizeTopicWordCloud("Hillary Clinton")

"""**Legalization of Abortion**"""

visualizeTopicWordCloud('Legalization of Abortion')

"""**Atheism**"""

visualizeTopicWordCloud('Atheism')

"""**Climate Change is a Real Concern**"""

visualizeTopicWordCloud('Climate Change is a Real Concern')

"""**Feminist Movement**"""

visualizeTopicWordCloud('Feminist Movement')

"""## Data Preparation

### Preparing Input & Target (x/y) values
"""

import string
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re

tweet_lines = list()
lines = train_data['Tweet'].values.tolist()

lines_length = list()
for line in lines:
    # Remove Punctuation except @ and #
    line = ' '.join(re.sub('([^0-9A-Za-z \t])|(\w+:\/\/\S+)', ' ', line).split())
    
    # tokenize the text
    tokens = word_tokenize(line)  
   
    # convert to lower case
    tokens = [w.lower() for w in tokens]

    # Append lines to calculate max length
    lines_length.append(len(tokens))

    # Append pre-processed tokens
    tweet_lines.append(tokens)

tweet_lines[:2]

max_length=max(lines_length)

max_length

from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences


tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(tweet_lines)
sequences = tokenizer_obj.texts_to_sequences(tweet_lines)

word_index = tokenizer_obj.word_index
print("unique tokens - "+str(len(word_index)))
vocab_size = len(tokenizer_obj.word_index) + 1
print('vocab_size - '+str(vocab_size))

tweet_lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')

tweet_lines_pad[:5]

train_data['Target'][:5]

target_lines = list()
lines = train_data['Target'].values.tolist()

target_lines_length = list()
for line in lines:
    # tokenize the text
    tokens = word_tokenize(line)

    target_lines_length.append(len(tokens))

    target_lines.append(tokens)

target_max_length=max(target_lines_length)

target_max_length

sequences = tokenizer_obj.texts_to_sequences(target_lines)

word_index = tokenizer_obj.word_index
print("unique tokens - "+str(len(word_index)))
vocab_size = len(tokenizer_obj.word_index) + 1
print('vocab_size - '+str(vocab_size))

target_lines_pad = pad_sequences(sequences, maxlen=target_max_length, padding='post')

target_lines_pad[:5]

print("Tweet Lines Shape", tweet_lines_pad.shape)
print("Target Lines Shape", target_lines_pad.shape)

from keras import utils as np_utils

stance_list = []

for value in train_data['Stance']:
  if value == "FAVOR":
    stance_list.append(0)
  elif value == "NONE":
    stance_list.append(1)
  elif value == "AGAINST":
    stance_list.append(2)


stance_list = np_utils.to_categorical(stance_list)

print("Tweet x Shape", tweet_lines_pad.shape )
print("Target x Shape", target_lines_pad.shape)
print("Stance y Shape", stance_list.shape)

"""### Preparing Test Dataset"""

# Removing Donald Trump Target Topic
test_data = test_data[test_data['Target'] != "Donald Trump"]

test_tweet_lines = list()
lines = test_data['Tweet'].values.tolist()

lines_length = list()
for line in lines:
    # Remove Punctuation except @ and #
    line = ' '.join(re.sub('([^0-9A-Za-z \t])|(\w+:\/\/\S+)', ' ', line).split())
    
    # tokenize the text
    tokens = word_tokenize(line)  
   
    # convert to lower case
    tokens = [w.lower() for w in tokens]
    
    lines_length.append(len(tokens))

    test_tweet_lines.append(tokens)

sequences = tokenizer_obj.texts_to_sequences(test_tweet_lines)

word_index = tokenizer_obj.word_index
print("unique tokens - "+str(len(word_index)))
vocab_size = len(tokenizer_obj.word_index) + 1
print('vocab_size - '+str(vocab_size))

test_tweet_lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')

# Check unique Target values for test data
test_data['Target'].unique()

test_target_lines = list()
lines = test_data['Target'].values.tolist()

lines_length = list()
for line in lines:
    # Remove Punctuation except @ and #
    line = ' '.join(re.sub('([^0-9A-Za-z \t])|(\w+:\/\/\S+)', ' ', line).split())
    
    # tokenize the text
    tokens = word_tokenize(line)  
   
    # convert to lower case
    tokens = [w.lower() for w in tokens]

    lines_length.append(len(tokens))

    test_target_lines.append(tokens)

sequences = tokenizer_obj.texts_to_sequences(test_target_lines)

word_index = tokenizer_obj.word_index
print("unique tokens - "+str(len(word_index)))
vocab_size = len(tokenizer_obj.word_index) + 1
print('vocab_size - '+str(vocab_size))

test_target_lines_pad = pad_sequences(sequences, maxlen=target_max_length, padding='post')

test_stance_list = []

for value in test_data['Stance']:
  if value == "FAVOR":
    test_stance_list.append(0)
  elif value == "NONE":
    test_stance_list.append(1)
  elif value == "AGAINST":
    test_stance_list.append(2)

test_stance_list = np_utils.to_categorical(test_stance_list)

print("Test Tweets Shape", test_tweet_lines_pad.shape)
print("Test Targets Shape",test_target_lines_pad.shape)
print("Test Stance Shape",test_stance_list.shape)

from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score
    
def getTestAccuracy(model):
    y_pred = model.predict([ test_tweet_lines_pad, test_target_lines_pad])
    
    y_pred_max = np.argmax(y_pred, axis=1)
    test_stance_list_max= np.argmax(test_stance_list, axis=1)

    return str("{:.2f}".format(accuracy_score(test_stance_list_max, y_pred_max) * 100))+"%"

def getF1Score(model):
    y_pred = model.predict([ test_tweet_lines_pad, test_target_lines_pad])
    
    y_pred_max = np.argmax(y_pred, axis=1)
    test_stance_list_max= np.argmax(test_stance_list, axis=1)

    return str("{:.2f}".format(f1_score(test_stance_list_max, y_pred_max, average="macro") * 100))+"%"

import seaborn as sns 

def getPredictionBarPlot(model, target):
  y_pred = model.predict([ test_tweet_lines_pad, test_target_lines_pad])
    
  y_pred_max = np.argmax(y_pred, axis=1)
  test_stance_list_max= np.argmax(test_stance_list, axis=1)

  index = 0
  rows_list = []

  counter = [0,0,0,0,0,0]

  for val in test_data["Target"]:
    if val == target:
      if y_pred_max[index] == 0:
        counter[0]+= 1
      elif y_pred_max[index] == 1:
        counter[1] += 1
      elif y_pred_max[index] == 2:
        counter[2] += 1
        
      if test_stance_list_max[index] == 0:
        counter[3] += 1
      elif test_stance_list_max[index] == 1:
        counter[4] += 1
      elif test_stance_list_max[index] == 2:
        counter[5] += 1
  
    index += 1

  rows_list.append({"type": "actual", "class": "favor", "count": counter[0]})
  rows_list.append({"type": "actual", "class": "neutral", "count": counter[1]})
  rows_list.append({"type": "actual", "class": "against", "count": counter[2]})
  rows_list.append({"type": "predicted", "class": "favor", "count": counter[3]})
  rows_list.append({"type": "predicted", "class": "neutral", "count": counter[4]})
  rows_list.append({"type": "predicted", "class": "against", "count": counter[5]})

  stats_df = pd.DataFrame(rows_list)  

  sns.barplot(x="class", y="count", hue="type", data=stats_df)
  plt.ylabel("Count", size=14)
  plt.xlabel("Class", size=14)
  plt.title("Predicted vs Actual: " + target, size=18)

"""### Importing GloVe Twitter Embedding Vector"""

# Check current file directory
!ls

# Import Preloaded Embedding Vectors (Pre-loaded and save in another notebook, for this particular shuffled train data)
E_T_25d = np.loadtxt("glove.twitter.25d.preloaded.txt")
E_T_100d = np.loadtxt("glove.twitter.100d.preloaded.txt")
E_T_200d = np.loadtxt("glove.twitter.200d.preloaded.txt")

# Check preloaded embedding vector shape.
print("25d preloaded vector shape:", E_T_25d.shape)
print("100d preloaded vector shape:", E_T_100d.shape)
print("200d preloaded vector shape:", E_T_200d.shape)

"""### Prepare Visualization Tools"""

from itertools import cycle
def plotter(history_hold, metric = 'loss', ylim=[0.0, 1.0]):
  cycol = cycle('bgrcmk')
  for name, item in history_hold.items():
    y_train = item.history[metric]
    y_val = item.history['val_' + metric]
    x_train = np.arange(0,len(y_val))

    c=next(cycol)

    plt.plot(x_train, y_train, c+'-', label=name+'_train')
    plt.plot(x_train, y_val, c+'--', label=name+'_val')

  plt.legend()
  plt.xlim([1, max(plt.xlim())])
  plt.ylim(ylim)
  plt.xlabel('Epoch')
  plt.ylabel(metric)
  plt.grid(True)

# plot the evolution of Loss and Acuracy on the train and validation sets
import matplotlib.pyplot as plt

def plotter_single(history):
    plt.figure(figsize=(20,10))
    plt.subplot(1, 2, 1)
    plt.suptitle('Optimizer : Adam', fontsize=10)
    plt.ylabel('Loss', fontsize=16)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.legend(loc='upper right')

    plt.subplot(1, 2, 2)
    plt.ylabel('Accuracy', fontsize=16)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.show()

def compare_params(metric="loss", h_histories= {}, params_list=[], param_name="Lambda", model_name="model_"):
  plt.figure(figsize=(10,5))
  l_train = list()
  l_val = list()

  for param in params_list:
    l_train.append(h_histories[model_name+ '_h' + str(param)].history[metric][-1])
    l_val.append(h_histories[model_name+ '_h' + str(param)].history['val_' + metric][-1])

  plt.plot(params_list,l_train, 'ro', label='Train' )
  plt.plot(params_list,l_val, 'bs', label='Test' )

  plt.xlabel(param_name, fontsize=14)
  plt.ylabel(metric, fontsize=14)
  plt.legend()
  plt.show()

"""## Model #1: Multi-input RNN Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense,Dropout,Embedding,Bidirectional,Input, Multiply, Concatenate

from tensorflow.keras import regularizers
from tensorflow.keras import Model
from tensorflow.keras import optimizers

from tensorflow.keras.models import Sequential

import tensorflow.keras.backend as K

from tensorflow.keras import initializers
from tensorflow.keras.layers import BatchNormalization

def optimizer_adam(dec_rate = 30,lr = 0.001, xtrain = None, batch_size = 32):
  STEPS_PER_EPOCH = xtrain.shape[0]
  lr_adam = lr

  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  lr_adam,
  decay_steps=STEPS_PER_EPOCH*1000,
  decay_rate=dec_rate,
  staircase=False)
  opt_adam = optimizers.Adam(learning_rate=lr_schedule)
  return opt_adam

def tiny_model(embedding_dim = 25, E_T = E_T_25d):
  ##########################
  # Model Inputs
  ##########################

  # define two sets of inputs (Tweets, Target Topic)
  # Input for Tweets.
  inputTweet = Input(shape=(tweet_lines_pad.shape[1],))

  # Input for Target Topic.
  inputTarget = Input(shape=(target_lines_pad.shape[1],))

  # Initalize Embedding Paramaters
  word_embedding_dim = embedding_dim
  nb_words = vocab_size
  input_tweet_length = tweet_lines_pad.shape[1]
  input_target_length = target_lines_pad.shape[1]

  # Number of Neurons in RNN Layer.
  x_RNN_dim = 32
  y_RNN_dim = 32
  

  ##########################
  # First Branch (Tweet)
  ##########################

  # the first branch (model x), that operates on the first input (input tweet)
  x = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTweet)

  x = Bidirectional(LSTM(x_RNN_dim, return_sequences=False))(x)

  x = Model(inputs=inputTweet, outputs=x)

  ##########################
  # Second Branch (Tweet)
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  y = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_target_length,
                      weights=[E_T],
                      trainable=False)(inputTarget)

  y = Bidirectional(LSTM(y_RNN_dim, return_sequences=False))(y)

  y = Model(inputs=inputTarget, outputs=y)

  ##########################
  # Merging of Branches
  ##########################

  # combine the output of the two branches (with Multiplication)
  combined = Concatenate()([x.output, y.output])



  ##########################
  # FC and Final Layer
  ##########################

  FC_dim = 64
  nb_classes = 3

  # apply a FC layer and then a softmax layer to predict the stance classes.
  z = Dense(FC_dim, activation="relu")(combined)
  
  z = Dense(nb_classes, activation="softmax")(z)



  ##########################
  # Model Building
  ##########################

  # Build combined model.
  model = Model(inputs=[x.input, y.input], outputs=z)

  
  ##########################
  # Model Compilation
  ##########################

  # Compile model
  lr = 0.001
  optimizer = optimizers.Adam(lr=lr)
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])
  
  return model

tiny_model_train = tiny_model()

# Define Fitting Parameters
VAL_SPLIT = 0.2
EPOCHS = 50
BATCH_SIZE = 32
VERBOSE = 1
SHUFFLE = True

history_tiny_model = tiny_model_train.fit(
                  x=[tweet_lines_pad,target_lines_pad],
                  y=stance_list,
                  validation_split=VAL_SPLIT,
                  epochs=EPOCHS,
                  batch_size=BATCH_SIZE,
                  verbose=VERBOSE,
                  shuffle=SHUFFLE)

plotter_single(history_tiny_model)

"""### Initial model with 25-dimensional embedding vector"""

def tiny_model_with_reg(embedding_dim = 25, dropout_rate = 0.4, lambda_val = 1e-4, E_T=E_T_25d):
  ##########################
  # Model Inputs
  ##########################

  # define two sets of inputs (Tweets, Target Topic)
  # Input for Tweets.
  inputTweet = Input(shape=(tweet_lines_pad.shape[1],))

  # Input for Target Topic.
  inputTarget = Input(shape=(target_lines_pad.shape[1],))

  # Initalize Embedding Paramaters
  word_embedding_dim = embedding_dim
  nb_words = vocab_size
  input_tweet_length = tweet_lines_pad.shape[1]
  input_target_length = target_lines_pad.shape[1]

  # Number of Neurons in RNN Layer.
  x_RNN_dim = 32
  y_RNN_dim = 32
  

  ##########################
  # First Branch (Tweet)
  ##########################

  # the first branch (model x), that operates on the first input (input tweet)
  x = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTweet)

  x = Dropout(dropout_rate)(x)

  x = Bidirectional(LSTM(x_RNN_dim,  
                                    return_sequences=False, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))(x)

  x = Model(inputs=inputTweet, outputs=x)

  ##########################
  # Second Branch (Tweet)
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  y = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_target_length,
                      weights=[E_T],
                      trainable=False)(inputTarget)

  y = Dropout(dropout_rate)(y)

  y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y_RNN_dim,  
                                    return_sequences=False, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))(y)

  y = Model(inputs=inputTarget, outputs=y)

  ##########################
  # Merging of Branches
  ##########################

  # combine the output of the two branches (with Multiplication)
  combined = tf.keras.layers.Concatenate()([x.output, y.output])



  ##########################
  # FC and Final Layer
  ##########################

  FC_dim = 64
  nb_classes = 3

  # apply a FC layer and then a softmax layer to predict the stance classes.
  z = Dense(FC_dim, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(combined)

  z = Dropout(dropout_rate)(z)

  z = Dense(nb_classes, activation="softmax")(z)



  ##########################
  # Model Building
  ##########################

  # Build combined model.
  model = Model(inputs=[x.input, y.input], outputs=z)

  
  ##########################
  # Model Compilation
  ##########################

  # Compile model
  lr = 0.001
  optimizer = optimizers.Adam(lr=lr)
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])
  
  return model

tiny_model_with_reg_train = tiny_model_with_reg()

# Define Fitting Parameters
VAL_SPLIT = 0.2
EPOCHS = 50
BATCH_SIZE = 32
VERBOSE = 2
SHUFFLE = True

history_tiny_model_with_reg = tiny_model_with_reg_train.fit(
                              x=[tweet_lines_pad,target_lines_pad],
                              y=stance_list,
                              validation_split=VAL_SPLIT,
                              epochs=EPOCHS,
                              batch_size=BATCH_SIZE,
                              verbose=VERBOSE,
                              shuffle=SHUFFLE)

plotter_single(history_tiny_model_with_reg)

histories = {}

histories["simple_model_without_reg"] = history_tiny_model
histories["simple_model_with_reg"] = history_tiny_model_with_reg
plotter(histories, ylim=[0.0,1.5])

plotter(histories,metric="accuracy", ylim=[0.4,0.8])

"""### Initial model with 100-dimensional embedding vector"""

tiny_model_with_reg_100d_train = tiny_model_with_reg(embedding_dim=100, E_T=E_T_100d)

# Define Fitting Parameters
VAL_SPLIT = 0.2
EPOCHS = 30
BATCH_SIZE = 32
VERBOSE = 1
SHUFFLE = True

history_tiny_model_with_reg_100d = tiny_model_with_reg_100d_train.fit(
        x=[tweet_lines_pad,target_lines_pad],
        y=stance_list,
        validation_split=VAL_SPLIT,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        verbose=VERBOSE,
        shuffle=SHUFFLE)

"""### Initial model with 200-dimensional embedding vector"""

tiny_model_with_reg_200d_train = tiny_model_with_reg(embedding_dim=200, E_T=E_T_200d)

# Define Fitting Parameters
VAL_SPLIT = 0.2
EPOCHS = 30
BATCH_SIZE = 32
VERBOSE = 1
SHUFFLE = True

history_tiny_model_with_reg_200d = tiny_model_with_reg_200d_train.fit(
        x=[tweet_lines_pad,target_lines_pad],
        y=stance_list,
        validation_split=VAL_SPLIT,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        verbose=VERBOSE,
        shuffle=SHUFFLE)

histories = {}

histories['initial_model_25d'] = history_tiny_model_with_reg
histories['initial_model_100d'] = history_tiny_model_with_reg_100d
histories['initial_model_200d'] = history_tiny_model_with_reg_200d

plotter(histories, ylim=[0.0,1.5])

plotter(histories,metric="accuracy", ylim=[0.0,1.0])

"""## Model #2: Multi-input RNN Model with Attention Layer

Attention layer source: https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/
"""

from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K

class attention(Layer):
    def __init__(self,**kwargs):
        super(attention,self).__init__(**kwargs)

    def build(self,input_shape):
        self.W=self.add_weight(name="att_weight",shape=(input_shape[-1],1),initializer="normal")
        self.b=self.add_weight(name="att_bias",shape=(input_shape[1],1),initializer="zeros")        
        super(attention, self).build(input_shape)

    def call(self,x):
        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)
        at=K.softmax(et)
        at=K.expand_dims(at,axis=-1)
        output=x*at
        return K.sum(output,axis=1)

    def compute_output_shape(self,input_shape):
        return (input_shape[0],input_shape[-1])

    def get_config(self):
        return super(attention,self).get_config()

def model_with_attention(embedding_dim = 25, dropout_rate = 0.45, lambda_val = 1e-4, E_T=E_T_25d):


  ##########################
  # Model Inputs
  ##########################

  # define two sets of inputs (Tweets, Target Topic)
  # Input for Tweets.
  inputTweet = Input(shape=(tweet_lines_pad.shape[1],))

  # Input for Target Topic.
  inputTarget = Input(shape=(target_lines_pad.shape[1],))

  # Initalize Embedding Paramaters
  word_embedding_dim = embedding_dim
  nb_words = vocab_size
  input_tweet_length = tweet_lines_pad.shape[1]
  input_target_length = target_lines_pad.shape[1]

  # Number of Neurons in RNN Layer.
  x_RNN_dim = 32
  y_RNN_dim = 32
  

  ##########################
  # First Branch (Tweet)
  ##########################

  # the first branch (model x), that operates on the first input (input tweet)
  x = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTweet)

  x = Dropout(dropout_rate)(x)

  x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x_RNN_dim,  
                                    return_sequences=True, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))(x)


  ##########################
  # Second Branch (Tweet)
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  y = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTarget)

  y = Dropout(dropout_rate)(y)

  y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y_RNN_dim,  
                                    return_sequences=True, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))(y)



  ##########################
  # Attention Layer
  ##########################
  
  attention_output_1 = attention()(x)
  attention_output_2 = attention()(y)

  ##########################
  # FC and Final Layer
  ##########################

  FC_dim = 64
  nb_classes = 3

  # apply two FC layer and then a softmax layer to predict the stance classes.
  z = Dense(FC_dim, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(attention_output_1)

  z = Dropout(dropout_rate)(z)
    
    
  q = Dense(FC_dim, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(attention_output_2)

  q = Dropout(dropout_rate)(q)
    
    
  ##########################
  # Merging of Streams
  ##########################
    
  combined = Concatenate()([z,q])


  finalOutput = Dense(nb_classes, activation="softmax")(combined)



  ##########################
  # Model Building
  ##########################

  # Build combined model.
  model = Model(inputs=[inputTweet, inputTarget], outputs=finalOutput)

  
  ##########################
  # Model Compilation
  ##########################

  # Compile model
  lr = 0.001
  optimizer = optimizers.Adam(lr=lr)
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])
  
  return model

model_with_attention_train = model_with_attention(embedding_dim=100, E_T=E_T_100d)

# Define Fitting Parameters
VAL_SPLIT = 0.25
EPOCHS = 80
BATCH_SIZE = 64
VERBOSE = 1
SHUFFLE = True

early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20,restore_best_weights=True, verbose=VERBOSE)
lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=15, verbose=VERBOSE, mode='auto')

CALLBACKS = [early_stopping]


history_model_with_attention = model_with_attention_train.fit(
        x=[tweet_lines_pad,target_lines_pad],
        y=stance_list,
        validation_split=VAL_SPLIT,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        verbose=VERBOSE,
        shuffle=SHUFFLE,
        callbacks=CALLBACKS)

plotter_single(history_model_with_attention)

"""## Model #3: Model with Tweet input conditioned with Target input"""

def model_conditional_encoding(embedding_dim = 25, dropout_rate = 0.45, lambda_val = 0.0025, E_T=E_T_100d, learning_rate=0.0004, batch_size = 32):
  ##########################
  # Model Inputs
  ##########################

  # define two sets of inputs (Tweets, Target Topic)
  # Input for Tweets.
  inputTweet = Input(shape=(tweet_lines_pad.shape[1],))

  # Input for Target Topic.
  inputTarget = Input(shape=(target_lines_pad.shape[1],))

  # Initalize Embedding Paramaters
  word_embedding_dim = embedding_dim
  nb_words = vocab_size
  input_tweet_length = tweet_lines_pad.shape[1]
  input_target_length = target_lines_pad.shape[1]

  # Number of Neurons in RNN Layer.
  x_RNN_dim = 128
  y_RNN_dim = 128
  

  ##########################
  # First Encoder (Tweet)
  ##########################

  # the first branch (model x), that operates on the first input (input target)
  x = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_target_length,
                      weights=[E_T],
                      trainable=False)(inputTarget)

  x = Dropout(dropout_rate)(x)

  target_encoding = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x_RNN_dim,  
                                    return_sequences=False, 
                                    return_state = True,
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))

  (target_encoding, target_forward_h, target_forward_s,target_backward_h, target_backward_s) = target_encoding(x)



  ##########################
  # Second Encoder (Tweet)
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  y = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTweet)

  y = Dropout(dropout_rate)(y)

  tweet_encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y_RNN_dim,  
                                    return_sequences=True,
                                    return_state = False, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))


  tweet_encoder_out = tweet_encoder(y, initial_state=[target_forward_h, target_forward_s,target_backward_h, target_backward_s])

  ##########################
  # Attention Layer
  ##########################
  attention_out = attention()(tweet_encoder_out)


  ##########################
  # FC and Final Layer
  ##########################

  FC_dim_1 = 256
  FC_dim_2 = 256
  nb_classes = 3

  # apply a FC layer and then a softmax layer to predict the stance classes.


  z = Dense(FC_dim_1, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(attention_out)

  z = Dropout(dropout_rate)(z)

  # apply a FC layer and then a softmax layer to predict the stance classes.
  z = Dense(FC_dim_2, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(z)

  z = Dropout(dropout_rate)(z)

  z = BatchNormalization()(z)

  z = Dense(nb_classes, activation="softmax")(z)



  ##########################
  # Model Building
  ##########################

  # Build combined model.
  model = Model(inputs=[inputTweet,inputTarget ], outputs=z)

  
  ##########################
  # Model Compilation
  ##########################

  # Compile model
  #lr = learning_rate
  optimizer = optimizer_adam(lr=learning_rate, xtrain=tweet_lines_pad, batch_size = batch_size )
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])
  
  return model

model_conditional_encoding_train = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d)

# Define Fitting Parameters
VAL_SPLIT = 0.2


MAX_EPOCHS = 200
BATCH_SIZE = 32
VERBOSE = 1
SHUFFLE = True

# Call backs (Reduce LR on plateau and finally stop early and save best weights)
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20,restore_best_weights=True, verbose=VERBOSE)
lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=8, verbose=VERBOSE, mode='auto', min_lr="0.0005")

CALLBACKS = [early_stopping, lr_reducer]


history_model_conditional_encoding = model_conditional_encoding_train.fit(
                  x=[tweet_lines_pad, target_lines_pad],
                  y=stance_list,
                  validation_split=VAL_SPLIT,
                  epochs=MAX_EPOCHS,
                  batch_size=BATCH_SIZE,
                  verbose=VERBOSE,
                  shuffle=SHUFFLE,
                  callbacks=CALLBACKS)

plotter_single(history_model_conditional_encoding)

histories = {}

histories["model_with_conditional_encoding"] = history_model_conditional_encoding

plotter(histories, metric="accuracy", ylim=[0.4,0.8])

plotter(histories, ylim=[0.7, 1.5])

"""## Model #4: Model with Target input conditioned with Tweet Input"""

def model_conditional_encoding_reverse(embedding_dim = 25, dropout_rate = 0.45, lambda_val = 0.0025, E_T=E_T_100d, learning_rate=0.0004, batch_size = 32):
  ##########################
  # Model Inputs
  ##########################

  # define two sets of inputs (Tweets, Target Topic)
  # Input for Tweets.
  inputTweet = Input(shape=(tweet_lines_pad.shape[1],))

  # Input for Target Topic.
  inputTarget = Input(shape=(target_lines_pad.shape[1],))

  # Initalize Embedding Paramaters
  word_embedding_dim = embedding_dim
  nb_words = vocab_size
  input_tweet_length = tweet_lines_pad.shape[1]
  input_target_length = target_lines_pad.shape[1]

  # Number of Neurons in RNN Layer.
  x_RNN_dim = 128
  y_RNN_dim = 128
  

  ##########################
  # First Encoder (Tweet)
  ##########################

  # the first branch (model x), that operates on the first input (input target)
  x = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTweet)

  x = Dropout(dropout_rate)(x)

  tweet_encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x_RNN_dim,  
                                    return_sequences=False, 
                                    return_state = True,
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))

  (tweet_encoder, tweet_forward_h, tweet_forward_s, tweet_backward_h, tweet_backward_s) = tweet_encoder(x)



  ##########################
  # Second Encoder (Target)
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  y = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_target_length,
                      weights=[E_T],
                      trainable=False)(inputTarget)

  y = Dropout(dropout_rate)(y)

  target_encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y_RNN_dim,  
                                    return_sequences=True,
                                    return_state = False, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))


  target_encoder_out = target_encoder(y, initial_state=[tweet_forward_h, tweet_forward_s, tweet_backward_h, tweet_backward_s])

  ##########################
  # Attention Layer
  ##########################
  attention_out = attention()(target_encoder_out)

  

  ##########################
  # FC and Final Layer
  ##########################

  FC_dim_1 = 256
  FC_dim_2 = 256
  nb_classes = 3

  # apply two FC layer and then a softmax layer to predict the stance classes.
  z = Dense(FC_dim_1, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(attention_out)

  z = Dropout(dropout_rate)(z)

  z = BatchNormalization()(z)

  z = Dense(FC_dim_2, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(z)

  z = Dropout(dropout_rate)(z)

  z = BatchNormalization()(z)

  z = Dense(nb_classes, activation="softmax")(z)



  ##########################
  # Model Building
  ##########################

  # Build combined model.
  model = Model(inputs=[inputTweet,inputTarget], outputs=z)

  
  ##########################
  # Model Compilation
  ##########################

  # Compile model
  optimizer = optimizer_adam(lr=learning_rate, xtrain=tweet_lines_pad, batch_size = batch_size )
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])
  
  return model

model_conditional_encoding_reverse_train = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d)

# Define Fitting Parameters
VAL_SPLIT = 0.2


MAX_EPOCHS = 200
BATCH_SIZE = 32
VERBOSE = 1
SHUFFLE = True

# Call backs (Reduce LR on plateau and finally stop early and save best weights)
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20,restore_best_weights=True, verbose=VERBOSE)
#lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=8, verbose=VERBOSE, mode='auto', min_lr="0.0005")

CALLBACKS = [early_stopping]


history_model_conditional_encoding_reverse = model_conditional_encoding_reverse_train.fit(
                  x=[tweet_lines_pad, target_lines_pad],
                  y=stance_list,
                  validation_split=VAL_SPLIT,
                  epochs=MAX_EPOCHS,
                  batch_size=BATCH_SIZE,
                  verbose=VERBOSE,
                  shuffle=SHUFFLE,
                  callbacks=CALLBACKS)

plotter_single(history_model_conditional_encoding_reverse)

"""## Model #5: Model with double conditional encoding"""

def model_double_conditional_encoding(embedding_dim = 25, dropout_rate = 0.45, lambda_val = 0.003, E_T=E_T_25d):


  ##########################
  # Model Inputs
  ##########################

  # define two sets of inputs (Tweets, Target Topic)
  # Input for Tweets.
  inputTweet = Input(shape=(tweet_lines_pad.shape[1],))

  # Input for Target Topic.
  inputTarget = Input(shape=(target_lines_pad.shape[1],))

  # Initalize Embedding Paramaters
  word_embedding_dim = embedding_dim
  nb_words = vocab_size
  input_tweet_length = tweet_lines_pad.shape[1]
  input_target_length = target_lines_pad.shape[1]

  # Number of Neurons in RNN Layer.
  x_RNN_dim = 128
  y_RNN_dim = 128
  

  ##########################
  # First Encoding (Target)
  ##########################

  # the first branch (model x), that operates on the first input (input target)
  x = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_target_length,
                      weights=[E_T],
                      trainable=False)(inputTarget)

  x = Dropout(dropout_rate)(x)

  target_encoding = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x_RNN_dim,  
                                    return_sequences=False, 
                                    return_state = True,
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))

  (target_encoding, target_forward_h, target_forward_s,target_backward_h, target_backward_s) = target_encoding(x)



  ##########################
  # First Encoding (Tweet)
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  y = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTweet)

  y = Dropout(dropout_rate)(y)

  tweet_encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x_RNN_dim,  
                                    return_sequences=False,
                                    return_state = True, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))


  (tweet_encoder, tweet_forward_h, tweet_forward_s,tweet_backward_h, tweet_backward_s) = tweet_encoder(y, initial_state=[target_forward_h, target_forward_s,target_backward_h, target_backward_s])


  ##########################
  # Second Encoding (Tweet)
  ##########################

  # the first branch (model x), that operates on the first input (input target)
  a = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_target_length,
                      weights=[E_T],
                      trainable=False)(inputTarget)

  a = Dropout(dropout_rate)(a)

  target_encoding_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y_RNN_dim,  
                                    return_sequences=False, 
                                    return_state = True,
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))

  (target_encoding_2, target_forward_h, target_forward_s,target_backward_h, target_backward_s) = target_encoding_2(a,initial_state=[tweet_forward_h, tweet_forward_s,tweet_backward_h, tweet_backward_s])



  ##########################
  # Second Encoding (Target)
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  b = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTweet)

  b = Dropout(dropout_rate)(b)

  tweet_encoder_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y_RNN_dim,  
                                    return_sequences=True,
                                    return_state = False, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))


  final_encoder_out = tweet_encoder_2(b, initial_state=[target_forward_h, target_forward_s,target_backward_h, target_backward_s])


  ##########################
  # Attention Layer
  ##########################

  attention_out = attention()(final_encoder_out)

  ##########################
  # FC and Final Layer
  ##########################

  FC_dim_1 = 256
  FC_dim_2 = 256
  nb_classes = 3


  # apply two FC layers and then a softmax layer to predict the stance classes.
  z = Dense(FC_dim_1, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(attention_out)

  z = Dropout(dropout_rate)(z)

  z = Dense(FC_dim_2, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(z)

  z = Dropout(dropout_rate)(z)
    
  z = BatchNormalization()(z)

  z = Dense(nb_classes, activation="softmax")(z)



  ##########################
  # Model Building
  ##########################

  # Build combined model.
  model = Model(inputs=[inputTweet, inputTarget], outputs=z)

    ##########################
  # Model Compilation
  ##########################

  # Compile model
  lr = 0.0005
  optimizer = optimizers.Adam(lr=lr)
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])
  

  return model

model_double_conditional_encoding_train = model_double_conditional_encoding(embedding_dim=200, E_T = E_T_200d)

# Define Fitting Parameters
VAL_SPLIT = 0.2
MAX_EPOCHS = 200
BATCH_SIZE = 32
VERBOSE = 1
SHUFFLE = True

# Call backs (Reduce LR on plateau and finally stop early and save best weights)
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20,restore_best_weights=True, verbose=VERBOSE)
lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=15, verbose=VERBOSE, mode='auto')

CALLBACKS = [early_stopping, lr_reducer]


history_model_double_conditional_encoding = model_double_conditional_encoding_train.fit(
                  x=[tweet_lines_pad, target_lines_pad],
                  y=stance_list,
                  validation_split=VAL_SPLIT,
                  epochs=MAX_EPOCHS,
                  batch_size=BATCH_SIZE,
                  verbose=VERBOSE,
                  shuffle=SHUFFLE,
                  callbacks=CALLBACKS)

plotter_single(history_model_double_conditional_encoding)

"""## Model #6: Model with two-way conditional encoding"""

def model_bi_conditional_encoding(embedding_dim = 25, dropout_rate = 0.45, lambda_val = 0.0025, learning_rate=0.00025, E_T = E_T_25d):
  
  ##########################
  # Model Inputs
  ##########################

  # define two sets of inputs (Tweets, Target Topic)
  # Input for Tweets.
  inputTweet = Input(shape=(tweet_lines_pad.shape[1],))

  # Input for Target Topic.
  inputTarget = Input(shape=(target_lines_pad.shape[1],))

  # Initalize Embedding Paramaters
  word_embedding_dim = embedding_dim
  nb_words = vocab_size
  input_tweet_length = tweet_lines_pad.shape[1]
  input_target_length = target_lines_pad.shape[1]

  # Number of Neurons in RNN Layer.
  x_RNN_dim = 128
  y_RNN_dim = 128
  

  ##########################
  # First Branch (Target)
  ##########################

  # the first branch (model x), that operates on the first input (input target)
  x = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_target_length,
                      weights=[E_T],
                      trainable=False)(inputTarget)

  x = Dropout(dropout_rate)(x)

  target_encoding = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x_RNN_dim,  
                                    return_sequences=False, 
                                    return_state = True,
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))

  (target_encoding, target_forward_h, target_forward_s,target_backward_h, target_backward_s) = target_encoding(x)



  ##########################
  # First Branch (Tweet)
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  y = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTweet)

  y = Dropout(dropout_rate)(y)

  tweet_encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x_RNN_dim,  
                                    return_sequences=True,
                                    return_state = False, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))


  first_encoder_out = tweet_encoder(y, initial_state=[target_forward_h, target_forward_s,target_backward_h, target_backward_s])


  ##########################
  # Second Branch (Tweet)
  ##########################

  # the first branch (model x), that operates on the first input (input target)
  a = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_tweet_length,
                      weights=[E_T],
                      trainable=False)(inputTweet)

  a = Dropout(dropout_rate)(a)

  tweet_encoding_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y_RNN_dim,  
                                    return_sequences=False, 
                                    return_state = True,
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))

  (tweet_encoding_2, tweet_forward_h, tweet_forward_s,tweet_backward_h, tweet_backward_s) = tweet_encoding_2(a)



  ##########################
  # Second Branch (Target)
  ##########################

  # the second branch (model y), that operates on the second input (input target)
  b = Embedding(output_dim=word_embedding_dim,
                      input_dim=nb_words,
                      input_length=input_target_length,
                      weights=[E_T],
                      trainable=False)(inputTarget)

  b = Dropout(dropout_rate)(b)

  target_encoder_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y_RNN_dim,  
                                    return_sequences=True,
                                    return_state = False, 
                                    kernel_regularizer=regularizers.l2(lambda_val),
                                    activity_regularizer=regularizers.l2(lambda_val),
                                    dropout=dropout_rate,
                                    recurrent_dropout=dropout_rate))


  final_encoder_out = target_encoder_2(b, initial_state=[tweet_forward_h, tweet_forward_s,tweet_backward_h, tweet_backward_s])


  ##########################
  # Attention Layers
  ##########################

  attention_out_1 = attention()(first_encoder_out)
  attention_out_2 = attention()(final_encoder_out)

  ##########################
  # FC and Final Layer
  ##########################

  FC_dim_1 = 256
  FC_dim_2 = 256
  nb_classes = 3


  # apply two FC layers before merging.
  z = Dense(FC_dim_1, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(attention_out_1)

  z = Dropout(dropout_rate)(z)

  z = Dense(FC_dim_2, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(z)

  z = Dropout(dropout_rate)(z)

  z = BatchNormalization()(z)

  # apply two FC layers before merging.
  q = Dense(FC_dim_1, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(attention_out_2)

  q = Dropout(dropout_rate)(q)

  q = Dense(FC_dim_2, activation="relu", kernel_regularizer=regularizers.l2(lambda_val))(q)

  q = Dropout(dropout_rate)(q)

  q = BatchNormalization()(q)


  # Merge two branches.
  combined = tf.keras.layers.Concatenate()([z,q])


  final_output = Dense(nb_classes, activation="softmax")(combined)



  ##########################
  # Model Building
  ##########################

  # Build combined model.
  model = Model(inputs=[ inputTweet, inputTarget], outputs=final_output)

  ##########################
  # Model Compilation
  ##########################

  # Compile model
  lr = learning_rate
  optimizer = optimizers.Adam(lr=lr)
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])
  

  return model

model_bi_conditional_encoding_train = model_bi_conditional_encoding(embedding_dim=200, E_T = E_T_200d, learning_rate=0.0003)

# Define Fitting Parameters
VAL_SPLIT = 0.2
MAX_EPOCHS = 200
BATCH_SIZE = 32
VERBOSE = 1
SHUFFLE = True

# Call backs (Reduce LR on plateau and finally stop early and save best weights)
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20,restore_best_weights=True, verbose=VERBOSE)
lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=8, verbose=VERBOSE, mode='auto', min_lr=0.0001)

CALLBACKS = [early_stopping, lr_reducer]


history_model_bi_conditional_encoding = model_bi_conditional_encoding_train.fit(
                  x=[tweet_lines_pad, target_lines_pad],
                  y=stance_list,
                  validation_split=VAL_SPLIT,
                  epochs=MAX_EPOCHS,
                  batch_size=BATCH_SIZE,
                  verbose=VERBOSE,
                  shuffle=SHUFFLE,
                  callbacks=CALLBACKS)

plotter_single(history_model_bi_conditional_encoding)

"""## Hyperparameter Search

In this section, a grid search approach will be used to find the most optimal hyperparameters for the best model architecture designed in this notebook:

The hyperparameters that will be searched are (in order of importance):


*   Learning Rate (alpha)
*   Dropout Rate
*   L2 Regularization (lambda)
*   Batch Size

### Learning Rate
"""

h_alpha_histories = {}

# Learning Rate Values (Previously determined desired range).
alpha_vals = [0.001, 0.0004, 0.0002, 0.0001]

# Perform grid search.
for val in alpha_vals:

  # Define Fitting Parameters
  VAL_SPLIT = 0.2
  MAX_EPOCHS = 80
  BATCH_SIZE = 32
  VERBOSE = 1
  SHUFFLE = True

  # Define model
  h_model = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d, learning_rate = val, batch_size=BATCH_SIZE)

  

  h_alpha_histories['model_ce_alpha'+ '_h' + str(val)] = h_model.fit(
                                                x=[tweet_lines_pad, target_lines_pad],
                                                y=stance_list,
                                                validation_split=VAL_SPLIT,
                                                epochs=MAX_EPOCHS,
                                                batch_size=BATCH_SIZE,
                                                verbose=VERBOSE,
                                                shuffle=SHUFFLE)

plotter(h_alpha_histories, ylim=[0.5,3])

plotter(h_alpha_histories, metric="accuracy", ylim=[0.4,0.8])

compare_params(params_list=alpha_vals,h_histories=h_alpha_histories, param_name="Alpha", model_name="model_ce_alpha")

compare_params(metric="accuracy",h_histories=h_alpha_histories, params_list=alpha_vals, param_name="Alpha", model_name="model_ce_alpha")

"""### Dropout Rate"""

h_dropout_histories = {}

# Dropout values.
dropout_vals = [0.3, 0.4, 0.45, 0.55]

best_learning_rate = 0.0004

# Perform grid search.
for val in dropout_vals:

  # Define Fitting Parameters
  VAL_SPLIT = 0.2

  MAX_EPOCHS = 80
  BATCH_SIZE = 32
  VERBOSE = 1
  SHUFFLE = True

  h_model = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d, learning_rate = best_learning_rate, dropout_rate = val, batch_size = BATCH_SIZE )

  

  h_dropout_histories['model_ce_dropout'+ '_h' + str(val)] = h_model.fit(
                                                x=[tweet_lines_pad, target_lines_pad],
                                                y=stance_list,
                                                validation_split=VAL_SPLIT,
                                                epochs=MAX_EPOCHS,
                                                batch_size=BATCH_SIZE,
                                                verbose=VERBOSE,
                                                shuffle=SHUFFLE)

plotter(h_dropout_histories, ylim=[0.5,3])

plotter(h_dropout_histories,metric="accuracy", ylim=[0.5,0.8])

compare_params(params_list=dropout_vals, h_histories=h_dropout_histories,param_name="Dropout Rate", model_name="model_ce_dropout")

compare_params(params_list=dropout_vals, h_histories=h_dropout_histories,param_name="Dropout Rate", model_name="model_ce_dropout", metric="accuracy")

"""### L2 Regularization"""

h_lambda_histories = {}

# L2 regularization values.
lambda_vals = [0.001, 0.0025, 0.005, 0.01]

best_learning_rate = 0.0004
best_dropout_rate = 0.45

# Perform grid search.
for val in lambda_vals:
  # Define Fitting Parameters
  VAL_SPLIT = 0.2
  MAX_EPOCHS = 80
  BATCH_SIZE = 32
  VERBOSE = 1
  SHUFFLE = True

  h_model = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d, learning_rate = best_learning_rate, dropout_rate=best_dropout_rate, lambda_val = val, batch_size = BATCH_SIZE)

  h_lambda_histories['model_ce_lambda'+ '_h' + str(val)] = h_model.fit(
                                                x=[tweet_lines_pad, target_lines_pad],
                                                y=stance_list,
                                                validation_split=VAL_SPLIT,
                                                epochs=MAX_EPOCHS,
                                                batch_size=BATCH_SIZE,
                                                verbose=VERBOSE,
                                                shuffle=SHUFFLE)

plotter(h_lambda_histories, ylim=[0.5,3])

plotter(h_lambda_histories,metric="accuracy", ylim=[0.5,0.8])

compare_params(params_list=lambda_vals, h_histories=h_lambda_histories,param_name="Lambda Val", model_name="model_ce_lambda")

compare_params(params_list=lambda_vals, h_histories=h_lambda_histories,param_name="Lambda Val", model_name="model_ce_lambda", metric="accuracy")

"""### Batch Size"""

h_batch_histories = {}

# Learning Rate Values.
batch_vals = [16, 32, 64]

best_learning_rate = 0.0004
best_dropout_rate = 0.45
best_lambda_val = 0.0025

# Perform grid search.
for val in batch_vals:

  # Define Fitting Parameters
  VAL_SPLIT = 0.2

  MAX_EPOCHS = 50
  BATCH_SIZE = val
  VERBOSE = 1
  SHUFFLE = True

  h_model = model_conditional_encoding(embedding_dim=200, E_T=E_T_200d, learning_rate = best_learning_rate, dropout_rate= best_dropout_rate, lambda_val= best_lambda_val, batch_size = BATCH_SIZE )

  h_batch_histories['model_ce_batch'+ '_h' + str(val)] = h_model.fit(
                                                x=[tweet_lines_pad, target_lines_pad],
                                                y=stance_list,
                                                validation_split=VAL_SPLIT,
                                                epochs=MAX_EPOCHS,
                                                batch_size=BATCH_SIZE,
                                                verbose=VERBOSE,
                                                shuffle=SHUFFLE)

plotter(h_batch_histories, ylim=[0.5,3])

plotter(h_batch_histories, metric="accuracy", ylim=[0.5,0.8])

compare_params(params_list=batch_vals, h_histories=h_batch_histories,param_name="Batch Size", model_name="model_ce_batch")

compare_params(params_list=batch_vals, h_histories=h_batch_histories,param_name="Batch Size", model_name="model_ce_batch", metric="accuracy")